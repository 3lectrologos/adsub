%%%% ijcai15.tex

\typeout{Non-monotone Adaptive Submodular Maximization}

% These are the instructions for authors for IJCAI-15.
% They are the same as the ones for IJCAI-11 with superficical wording
%   changes only.

\documentclass{article}
% The file ijcai15.sty is the style file for IJCAI-15 (same as ijcai07.sty).
\usepackage{ijcai15}

% Use the postscript times font!
\usepackage{times}

% the following package is optional:
%\usepackage{latexsym} 

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

%-------------------------------------------------------------------------------
% Custom definitions
%-------------------------------------------------------------------------------
\usepackage{etoolbox}
% Define output mode (default: six-page IJCAI)
\newtoggle{short}
\toggletrue{short}
% Uncomment for long version
\togglefalse{short}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}                    % For \vcentcolon
\usepackage{xspace}            % Controls space after user-defined command
\usepackage{xparse}            % Optional arguments in commands
\usepackage{enumitem}
\usepackage{fixltx2e}          % For subscripts in normal text
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pgfplots}
\iftoggle{short}
{
\usepackage{nohyperref}
\usepackage{url}
}
{
\usepackage{flushend}
\usepackage[debug]{hyperref}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{%
  pdftitle={Active Learning for Level Set Estimation},
  pdfauthor={},
  pdfsubject={},
  pdfkeywords={},
  pdfborder=0 0 0,
  pdfpagemode=UseNone,
  colorlinks=true,
  linkcolor=mydarkblue,
  citecolor=mydarkblue,
  filecolor=mydarkblue,
  urlcolor=mydarkblue,
  pdfview=FitH}
}

\newcommand{\todo}[1]{\noindent\texttt{\small\color[rgb]{0.5,0.1,0.1} TODO: #1}}

% Refs
\newcommand{\sectref}[1]{\hyperref[#1]{Section \ref*{#1}}}
\newcommand{\chapref}[1]{\hyperref[#1]{Chapter \ref*{#1}}}
\newcommand{\figref}[1]{\hyperref[#1]{Figure \ref*{#1}}}
\newcommand{\figsref}[1]{\hyperref[#1]{Figures \ref*{#1}}}
\newcommand{\tabref}[1]{\hyperref[#1]{ Table \ref*{#1}}}
\newcommand{\algoref}[1]{\hyperref[#1]{Algorithm \ref*{#1}}}
\newcommand{\theoremref}[1]{\hyperref[#1]{Theorem \ref*{#1}}}
\newcommand{\lemmaref}[1]{\hyperref[#1]{Lemma \ref*{#1}}}
\newcommand{\lemmasref}[1]{\hyperref[#1]{Lemmas \ref*{#1}}}
\newcommand{\corref}[1]{\hyperref[#1]{Corollary \ref*{#1}}}
\newcommand{\asref}[1]{\hyperref[#1]{Assumption \ref*{#1}}}
\newcommand{\eqtref}[1]{\hyperref[#1]{\mbox{(\ref*{#1})}}}
\newcommand{\appref}[1]{\hyperref[#1]{Appendix \ref*{#1}}}
\newcommand{\lineref}[1]{\hyperref[#1]{line \ref*{#1}}}
\newcommand{\linesref}[2]{\hyperref[#1]{lines \ref*{#1}--\ref*{#2}}}
\newcommand{\linsref}[1]{\hyperref[#1]{lines \ref*{#1}}}

% Theory environments
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{assumption}{Assumption}

% Algorithm-related
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand*\LNot{\textbf{not}\xspace}
\newcommand*\LAnd{\textbf{and}\xspace}
\newcommand*\LET[2]{\STATE #1 $\gets$ #2}
\newcommand*\Fcall[1]{\textsc{#1}}
\makeatletter
\newcommand{\setalglineno}[1]{%
  \setcounter{ALC@line}{\numexpr#1-1}}
\makeatother
\renewcommand{\algorithmiccomment}[1]{// #1}
\newcommand{\LINEIF}[2]{%
    \STATE\algorithmicif\ {#1}\ \algorithmicthen\ {#2}%
}
\newcommand{\LINEELSE}[1]{%
    \STATE\algorithmicelse\ {#1}%
}

% Math definitions
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\def\*#1{\bm{#1}}

\newcommand{\twopartdef}[4]
{
	\left\{
		\begin{array}{ll}
			#1\,,& \mbox{if } #2 \\
			#3\,,& \mbox{if } #4
		\end{array}
	\right.
}

\newcommand{\twopartdefo}[3]
{
	\left\{
		\begin{array}{ll}
			#1\,,& \mbox{if } #2 \\
			#3\,,& \mbox{otherwise}
		\end{array}
	\right.
}

\newcommand{\defeq}{\vcentcolon=}

% Fix spacing problem with \left and \right
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

% Math definitions
\newcommand{\smid}{\ \middle\vert\ }
\newcommand{\mmid}{\,\vert\,}

\DeclareDocumentCommand \E { o m o }{%
  \IfValueTF {#1} {%
    \mathbb{E}_{#1}
  }{%
    \mathbb{E}
  }%
  \left[#2
  \IfValueT {#3} {%
    \smid #3
  }%
  \right]
}

\DeclareDocumentCommand \P { o m o }{%
  \IfValueTF {#1} {%
    \mathbb{P}_{#1}
  }{%
    \mathbb{P}
  }%
  \left[#2
  \IfValueT {#3} {%
    \smid #3
  }%
  \right]
}

\newcommand{\D}[2]{\Delta(#1\mmid#2)}
\newcommand{\sdef}[2]{\left\{#1\smid#2\right\}}

\newcommand{\pio}{\pi_{[0]}}
\newcommand{\pii}{\pi_{[i]}}
\newcommand{\pik}{\pi_{[k]}}
\newcommand{\pis}{\pi^*}
\newcommand{\pisi}{\pi^*_{[i]}}
\newcommand{\pisk}{\pi^*_{[k]}}
\newcommand{\pigr}{\pi^{\textrm{g}}}
\newcommand{\pig}{\pi^{\textrm{rg}}}
\newcommand{\pigo}{\pi^{\textrm{rg}}_{[0]}}
\newcommand{\pigi}{\pi^{\textrm{rg}}_{[i]}}
\newcommand{\pigl}{\pi^{\textrm{rg}}_{[\ell]}}
\newcommand{\pigii}{\pi^{\textrm{rg}}_{[i+1]}}
\newcommand{\pigk}{\pi^{\textrm{rg}}_{[k]}}
\newcommand{\favg}{f_{\mathrm{avg}}}
\newcommand{\dom}{\mathcal{D}}
\newcommand{\Mk}{\mathcal{M}_k}

\newcommand{\uitem}[1]{\item[#1]}

% For citations like: Author et. al [2010]
\newcommand{\citet}[1]{\citeauthor{#1}~\shortcite{#1}}

% Paragraph
\renewcommand{\paragraph}[1]{\vspace{0.3em}\noindent\textbf{#1.}\makebox[0.5em]{}}
%-------------------------------------------------------------------------------

\title{Non-monotone Adaptive Submodular Maximization}
\author{Alkis Gotovos\\
ETH Zurich
\And
Amin Karbasi\\
Yale University
\And
Andreas Krause\\
ETH Zurich}

\begin{document}

\maketitle

\begin{abstract}
A wide range of AI problems require us to sequentially select elements from a large set with the goal of optimizing some form of utility of our selected subset.
Moreover, each element we pick may provide us with stochastic feedback, which we can use to make smarter decisions about future selections.
While finding efficient policies for this general class of adaptive stochastic optimization problems can be extremely hard, the subclass of problems, for which the objective function is adaptive monotone and adaptive submodular, has been shown to possess a number of desirable properties; most notably, the simple and efficient greedy policy obtains a $1-1/e$ approximation ratio.
However, many objectives of practical interest are naturally non-monotone; as a simple example, each selected element may incur some cost that needs to be subtracted from the original monotone utility, resulting in a non-monotone objective.
For these objectives, there are no existing policies with provable performance guarantees.
We propose the \emph{random greedy policy} for maximizing adaptive submodular functions, and prove that it retains the aforementioned $1-1/e$ approximation ratio for functions that are also adaptive monotone, while it additionally provides a $1/e$ ratio for non-monotone functions.
We showcase the benefits of our proposed policy on several small-scale network data sets using two non-monotone objectives.
\end{abstract}

\section{Introduction}

\section{Related work}
\paragraph{Non-monotone submodular maximization}\\
\paragraph{Adaptive submodularity}\\
\paragraph{Influence maximization and max-cut}

\clearpage
\section{Problem Statement and Background}
Assume we are given a finite ground set $E$ and a set $O$ of observable states.
Each item $e \in E$ is associated with a state $o \in O$ through a function $\phi : E \to O$, which is called a realization of the ground set.
In our setting, we assume that the realization $\Phi$ is a random variable with known distribution $p(\phi)$.
Furthermore, we are given an objective function $f : 2^E \times O^E \to \mathbb{R}_{\geq 0}$.
For a set $A \subseteq E$ and a realization $\phi$, the quantity $f(A, \phi)$ represents the utility of selecting subset $A$ when the true realization is $\phi$.

Our goal is to come up with a sequential policy that builds up a set $A \subseteq E$, such that our utility $f(A, \Phi)$ is maximized.
That is, we iteratively select an item $e \in E$ to add to $A$ and observe its state $\Phi(e)$.
In this setting, there are two factors that complicate matters compared to its non-adaptive counterpart.
First, since our utility depends on the (random) realization, we need to maximize the expected utility under the distribution of realizations $p(\phi)$.
Second, the chosen set $A$ itself is a random variable that depends on the realization, since the choices of our policy will change according to each observation $\Phi(e)$, which is, of course, the whole point of adaptivity.
In addition, the policy itself might make random decisions, which is an additional source of randomness for $A$.

To address the above complications, we define a partial realization as a set $ \psi \subseteq E \times O$, which represents the item-observation pairs over a subset of $E$.
In particular, we call this subset the \emph{domain} of $\psi$, which is formally defined as $\dom(\psi) \defeq \sdef{e \in E}{\exists o \in O : (e, o) \in \psi}$.
Additionally, we write $\psi(e) = o$, if $(e, o) \in \psi$, and call $\psi$ \emph{consistent} with realization $\phi$ (denoted by $\phi \sim \psi$), if $\psi(e) = \phi(e)$, for all $e \in \dom(\psi)$, which means that the observations of a subset according to $\psi$ agree with the assignments over the whole ground set according to $\phi$.

Now, we can define a \emph{policy} $\pi$ as a function from partial realizations to a distribution over which item to pick next, formally, $\pi : 2^{E \times O} \to \mathcal{P}(E)$.
The policy terminates when the current partial realization is not in its domain denoted by $\dom(\pi) \subseteq 2^{E \times O}$.
We use the notation $\pi(e\mmid\psi)$ for the probability of picking item $e$ given partial realization $\psi$.
We call $E(\pi, \Phi) \subseteq E$ the set of items that have been selected upon termination of policy $\pi$ under realization $\Phi$.
Note that this set a random variable that depends on both the randomness of the policy, as well as the randomness of the realization.

Finally, we can formally assess the performance of a policy $\pi$ via its expected utility,
\begin{align*}
  \favg(\pi) \defeq \E[\Phi,\Pi]{f(E(\pi, \Phi), \Phi)}.
\end{align*}
Then, our goal is to come up with a policy that maximizes the expected utility, subject to a cardinality constraint on the number of items to be picked, $|E(\pi, \Phi)| \leq k$.

\subsection{Monotonicity and Submodularity}
\paragraph{Non-adaptive}
Even in the non-adaptive setting, where the realization is fixed and known in advance, or equivalently, the objective function $f : 2^E \to \mathbb{R}_{\geq 0}$ depends only on the chosen subset, the problem of maximizing $f(A)$, subject to a cardinality constraint $|A| \leq k$, is NP-hard in general.
In this setting, the marginal gain of an element $e \in E$ given set $B \subseteq E$ is defined as $f(B \cup \{e\}) - f(B)$.
Intuitively, the marginal gain quantifies the increase in utility if we add $e$ to our selection, given that we have already picked the elements in $B$.
Function $f$ is called monotone if for any $B \subseteq C \subseteq E$, it holds that $f(B) \leq f(C)$, which is equivalent to saying that the marginal gain is always positive.
Furthermore, $f$ is called submodular, if for any $B \subseteq C \subseteq E$ and any $e \in E \setminus C$, it holds that $f(C \cup \{e\}) - f(C) \leq  f(B \cup \{e\}) - f(B)$.
This means that the marginal gain of any element decreases as the given set increases ($C \supseteq B$); in other words, it expresses a behavior of ``diminishing returns''.

In their famous result, \citet{nemhauser78} showed that, if $f$ is non-negative, monotone, and submodular, then constructing a subset $A$ by picking elements greedily according to their marginal gains, guarantees that $f(A)$ is a ($1 - 1/e$)-approximation to the optimal value.

\paragraph{Adaptive}
In the significantly more complex adaptive setting, the problem of computing an optimal policy is hard to approximate even for seemingly simple classes of objective functions (e.g. linear), as shown by \citet{golovin11}.
However, they also showed that the notions of monotonicity and submodularity can be naturally generalized to this setting, and be used to give similar performance guarantees as in the non-adaptive case.

More concretely, the expected marginal gain of element $e \in E$ given partial realization $\psi$ can be defined as
\begin{align*}
  \D{e}{\psi} \defeq \E[\Phi]{f\big(\mathcal{D}(\psi) \cup \{e\}, \Phi\big) - f\big(\dom(\psi), \Phi\big)}[\Phi \sim \psi].
\end{align*}
The above expression is a conditional expectation, which only considers realizations that are consistent with $\psi$.
Then, the following properties can be defined analogously to their non-adaptive counterparts:
\begin{itemize}
\item $f$ is called adaptive monotone, if $\D{e}{\psi} \geq 0$, for all $e \in E$ and all $\psi$ of positive probability,
\item $f$ is called adaptive submodular, if $\D{e}{\psi'} \leq \D{e}{\psi}$, for all $e \in E \setminus \dom(\psi')$ and all $\psi' \supseteq \psi$.
\end{itemize}
A stronger condition, which is often much easier to verify in practice, is that $f(\cdot, \phi)$ be submodular for any $\phi$, which immediately implies adaptive submodularity of $f$ for any distribution $p(\phi)$.

Given a number of previously selected elements and their corresponding observed states encoded in a partial realization $\psi$, the adaptive greedy policy $\pigr$ selects the element $e \in E \setminus \psi$ that achieves the highest marginal gain $\D{e}{\psi}$, and does so iteratively until $k$ elements have been selected.
\citet{golovin11} showed that under the assumption of adaptive monotonicity and submodularity, $\pigr$ is a $(1-1/e)$-approximation to the optimal policy in terms of expected utility $\favg$.

\subsection{Non-monotone functions}
While many functions of interest have been shown to satisfy adaptive monotonicity and submodularity, there is a number of notable objectives that are adaptive submodular, but non-monotone. \todo{refs?}
In particular, there are two classes of adaptive submodular functions, for which non-monotonicity results naturally from their definition.
First, functions that directly incorporate a cost term in their definition, for example, of the form $f(A, \phi) = f_{\textrm{utility}}(A, \phi) - f_{\textrm{cost}}(A)$, where $f_{\textrm{utility}}$ is a monotone, submodular function.
Even for $f_{\textrm{cost}}$ being a simple modular function, such as $f_{\textrm{cost}}(A) = |A|$, the resulting objective is submodular, but non-monotone.
Second, symmetric submodular functions, that is, submodular functions that satisfy $f(A, \phi) = f(E \setminus A, \phi)$, for all $A \subseteq E$, which are by definition non-monotone.

\section{Adaptive Random Greedy}
We now present our proposed adaptive random greedy policy ($\pig$) for maximizing (non-monotone) adaptive submodular functions.
Much like the adaptive greedy policy generalized the non-adaptive greedy algorithm, we generalize the recently proposed non-adaptive random greedy algorithm \cite{buchbinder14} to the adaptive setting.

For technical reasons that will become apparent below, let us assume that we always add a set $D$ of $2k - 1$ dummy elements to the ground set, such that, for any $d \in D$, and any partial realization $\psi$, it holds that $\D{d}{\psi} = 0$.
Obviously, these elements do not affect the optimal policy, and may be removed from the solution of any policy, without affecting its expected utility.

The detailed pseudocode of using adaptive random greedy is presented \algoref{alg:rg}.
As discussed before, we are given a ground set and an objective function, as well as a known distribution over realizations $\Phi$.
After computing the marginal gains
The key difference compared to the adaptive greedy policy is shown in \linesref{lin:upd1}{lin:upd2} of the algorithm.
Rather than selecting the element with the largest expected marginal gain, $\pig$ randomly selects an element among the ones with the $k$ largest gains.
The dummy elements added to the ground set ensure that the policy never picks an element with negative expected marginal gain.
Also, note that, although in \algoref{alg:rg} the returned set $A$ contains exactly $k$ elements, since we remove any dummy elements from it, the actual selected set may very well contain less than $k$ elements.

When running the original adaptive greedy policy, non-monotonicity can lead to situations, where selecting the element of maximum marginal gain leads to traps of low utility that cannot be escaped by that policy.
In contrast, the randomness introduced by adaptive random greedy to the selection of each element helps dealing with such traps (on average) and, thus, leads to guaranteed approximation guarantees for the expected utility, even for non-monotone objectives.

\begin{algorithm}[tb]
  \caption{Adaptive random greedy}
  \label{alg:rg}
  \normalsize{
    \begin{algorithmic}[1]
      \REQUIRE ground set $E$, function $f$, distribution $p(\phi)$, cardinality constraint $k$
      %  \ENSURE set $S_k \subseteq E$
      \LET{$A$}{$\varnothing$}
      \LET{$\psi$}{$\varnothing$}
      \FOR{$i = 1$ \TO $k$}
      \STATE Compute $\D{e}{\psi}$, for all $e \in E$ \label{lin:marg}
      \LET{$\Mk(\psi)$}{$\displaystyle\argmax_{S \subseteq E \setminus A,\,|S| = k}\left\{\sum_{e \in S} \D{e}{\psi} \right\}$} \label{lin:upd1}
      \STATE Sample $m$ uniformly at random from $\Mk(\psi)$ \label{lin:upd2}
      \LET{$A$}{$A \cup \{m\}$}
      \STATE Observe $\Phi(m)$
      \LET{$\psi$}{$\psi \cup \left\{\big(m, \Phi(m)\big)\right\}$}
      \ENDFOR
      \STATE Return $A$
    \end{algorithmic}
  }
\end{algorithm}

\paragraph{Theoretical analysis}
More concretely, we now show that the adaptive random greedy policy retains the $1-1/e$ approximation ratio for adaptive monotone submodular objectives, while, at the same time, it achieves a $1/e$ approximation ratio for non-monotone objectives, under the somewhat stronger condition of submodularity for each realization.
As stated in the previous section, this stronger condition implies adaptive submodularity and, in fact, is satisfied in the majority of practical applications, since it is the most common way to prove adaptive submodularity in the first place.
Therefore, we do not consider it a major restriction in the choice of objective functions.
Also, note that our proofs generalize the results of \cite{buchbinder14} for the random greedy algorithm.

Detailed proofs can be found in the longer version of this paper; in what follows, we provide an outline of our analysis.
First, let us define the expected gain of running policy $\pi$ after having obtained a partial realization $\psi$, as
{\small
\begin{align*}
  \D{\pi}{\psi} \defeq \E[\Phi,\Pi]{f(\dom(\psi) \cup E(\pi, \Phi), \Phi) - f(\dom(\psi), \Phi)}[\Phi \sim \psi].
\end{align*}}
Also, for any policy $\pi$, we define a truncated policy $\pik$, which runs indentically to $\pi$ for $k$ steps and then terminates.
Finally, given any two policies $\pi_1$, $\pi_2$, we denote by $\pi_1@\pi_2$ a new policy that first runs $\pi_1$ until it terminates and then continues with $\pi_2$, discarding the observations made by $\pi_1$.
In the end, the subset selected by $\pi_1@\pi_2$ will be the union of the subsets selected by each policy individually.

The following is a key lemma for both monotone and non-monotone objectives.
\setcounter{lemma}{0}
\begin{lemma}
  If $f$ is adaptive submodular, then, for any policy $\pi$, and any partial realization $\psi$, it holds that
  \begin{align*}
    \D{\pik}{\psi} \leq \sum_{e \in \Mk(\psi)} \D{e}{\psi}.
  \end{align*}
\end{lemma}
\noindent It states the intuitive fact that, at any point, no $k$-step policy can give us a larger expected gain than the sum of the $k$ currently largest expected marginal gains.
This is a consequence of adaptive submodularity, which guaratees that the expected marginal gains of any element will decrease as our selection grows larger.

Based on \lemmaref{lem:submod} and the fact that $\pig$ selects at each step one of the $k$ elements in $\Mk$ uniformly at random, we can show the following lemma, which also applies to both monotone and non-monotone objectives.
\begin{lemma}\label{lem:mon_main}
  For any policy $\pi$ and any non-negative integer $i < k$, if $f$ is adaptive submodular, then
  \begin{align*}
    \favg(\pigii) - \favg(\pigi) \geq \frac{1}{k}\left(\favg(\pigi @ \pi) - \favg(\pigi)\right).
  \end{align*}
\end{lemma}
\noindent The lemma compares the expected gain at the $i$-th step of $\pig$ to the total gain of running any other policy (e.g., the optimal one) after the $i$-th step and, thereby, provides a means for obtaining approximation guarantees for $\pig$, as long as we can bound the term $\favg(\pigi @ \pi)$.

If $f$ is adaptive monotone, we may use the bound $\favg(\pigi @ \pi) \geq \favg(\pi)$, which readily follows from the definition of adaptive monotonicity, to obtain the following theorem.
\begin{theorem}
  If $f$ is adaptive monotone submodular, then for any policy $\pi$ and all integers $i, k > 0$ it holds that
  \begin{align*}
    \favg(\pigi) \geq \left(1 - e^{-i/k}\right)\favg(\pik).
  \end{align*}
\end{theorem}
\noindent In particular, by setting $i = k$ we get the familiar $1-1/e$ approximation ratio for $\pigk$.

For the non-monotone case, we need to leverage the randomness of the selection process of $\pig$ to bound $\favg(\pigi @ \pi)$.
For that purpose, we generalize to the adaptive setting the following lemma shown by \citet{buchbinder14}, which itself is based on a lemma by \citet{feige07} for the expected value of a submodular function under a randomly selected subset.
\begin{lemma}[\cite{buchbinder14}]
  If $f : 2^E \to \mathbb{R}_{\geq 0}$ is submodular and $A$ is a random subset of $E$, such that each element $e \in E$ is contained in $A$ with probability at most $p$, that is, $\P[A]{e \in A} \leq p,\ \forall e \in E$, then
  \begin{align*}
    \E[A]{f(A)} \geq (1-p)f(\varnothing).
  \end{align*}
\end{lemma}
\noindent Roughly speaking, the lemma states that a ``random enough'' subset $A$ cannot have much worse value than that of the empty set.
Note that $f$ here is not assumed to be monotone.

The following lemma extends the above claim to the adaptive setting.
\begin{lemma}
  If $f(\cdot\,, \phi) : 2^E \to \mathbb{R}_{\geq 0}$ is submodular for all $\phi \in O^E$, then for any policy $\pi$ such that each element of $e \in E$ is selected by it with probability at most $p$, that is, $\P[\Pi]{e \in E(\pi, \phi)} \leq p,\ \forall \phi \in O^E,\ \forall e \in E$, the expected value of running $\pi$ can be bounded as follows:
\begin{align*}
  \favg(\pi) \geq (1-p)\,\favg(\pio).
\end{align*}
\end{lemma}
\noindent Although we have not found concrete examples of adaptive submodular functions that do not satisfy this lemma, adaptive submodularity by itself does not seem to be a sufficient condition for it to hold; rather, we require the stronger assumption that $f$ be submodular for any realization.

As a consequence of the above lemma, we get that $\favg(\pigi@\pi) = \favg(\pi@\pigi) \geq (1-p)\favg(\pi)$, meaning that the elements added by adaptive random greedy cannot reduce the average value obtained by any other policy $\pi$ by too much.
The probability $p$ in this case can be bounded by using the fact that $\pig$ randomly selects one of $k$ elements at each step, therefore at the $i$-step we have $p \leq (1-1/k)^i$.
Putting it all together, we obtain the following theorem for non-monotone objectives.
\begin{theorem}\label{thm:nonm}
  If $f(\cdot\,, \phi) : 2^E \to \mathbb{R}_{\geq 0}$ is submodular for all $\phi \in O^E$, then, for any policy $\pi$, and all integers $i, k > 0$, it holds that
  \begin{align*}
    \favg(\pigi) \geq \frac{i}{k}\left(1 - \frac{1}{k}\right)^{i-1}\favg(\pik).
  \end{align*}
\end{theorem}
\noindent By setting $i = k$ we get a $1/e$ approximation ratio for $\pigk$.


\section{Experiments}
We have evaluated our proposed algorithm on two objective functions using both synthetic and real-world data sets.
Since we have no competitor policy for the adaptive non-monotone submodular setting, we rather focus here on showcasing the potential benefits of adaptivity by comparing adaptive to non-adaptive random greedy on two objectives that are directly applicable to both settings and satisfy the assumptions of \theoremref{thm:nonm}.

\subsection{Objective Functions}
\paragraph{Influence maximization}
The problem of influence maximization in a social network was posed by \citet{kempe03}.
Given a graph and a model of influence propagation, the goal is to select a subset of nodes that maximizes the expected number of nodes that will ultimately be active or influenced according to the propagation model.
We focus here on the independent cascade model, for which the problem setup is as follows.
The ground set $E$ consists of network nodes and each realization $\Phi$ corresponds to a full ``outcome'' of the independent cascade model, that is, a boolean assignment to each network edge being either ``live'' or ``blocked''.
Then, the objective function $f_{\textrm{inf}}(A, \Phi)$ is the number of active nodes under realization $\Phi$, if subset $A$ is initially active.
\citet{kempe03} showed that this function is non-negative, monotone, and submodular, for any realization $\Phi$.

\citet{golovin11} formulated an adaptive version of this problem, in which, each selection of a node $v$ reveals the status (``live'' or ``blocked'') of all outgoing edges of $v$ and of any other node that can be reached from $v$ through ``live'' edges.
Furthermore, they showed that the above objective is adaptive monotone submodular.

For our experiments, we assume that each selected node incurs a unit cost, or, equivalently, we subtract a modular term from the original influence objective,
\begin{align*}
  f_1(A, \Phi) \defeq f_{\textrm{inf}}(A, \Phi) - |A|,
\end{align*}
which results in a new non-negative, non-monotone, submodular objective for any realization $\Phi$.

\paragraph{Maximum graph cut}
The problem of finding the maximum cut in a graph is a much studied NP-complete problem with various proposed algorithms. \todo{refs}
Interestingly, the maximum cut objective is non-negative, symmetric submodular, and, therefore, non-monotone.

For our experiments, we consider a modified version of this problem, in which, selecting a node results in cutting that node or either of its neighbors with equal probability.
The ground set $E$ consists again of network nodes and each realization $\Phi$ corresponds to a function $\sigma_{\Phi}$ that maps each node $v \in V$ to the node that is actually cut if $v$ is selected.
If we call the classic maximum cut objective $f_{\textrm{cut}}$, then our modified objective can be written as follows:
\begin{align*}
  f_2(A, \Phi) \defeq f_{\textrm{cut}}\left(\bigcup_{v \in A}\sigma_{\Phi}(v)\right).
\end{align*}
The new objective is also non-negative, non-monotone, and submodular, for any realization $\Phi$.

\subsection{Data sets}

\subsection{Results}

\section{Conclusion}

\clearpage
% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai15}

\iftoggle{short}
{}
{
\clearpage
\onecolumn
\appendix
\setcounter{lemma}{0}
\setcounter{theorem}{0}
\include{proofs}
}

\end{document}
