%%%% ijcai15.tex

\typeout{Non-monotone Adaptive Submodular Maximization}

% These are the instructions for authors for IJCAI-15.
% They are the same as the ones for IJCAI-11 with superficical wording
%   changes only.

\documentclass{article}
% The file ijcai15.sty is the style file for IJCAI-15 (same as ijcai07.sty).
\usepackage{ijcai15}

% Use the postscript times font!
\usepackage{times}

% the following package is optional:
%\usepackage{latexsym} 

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

%-------------------------------------------------------------------------------
% Custom definitions
%-------------------------------------------------------------------------------
\usepackage{etoolbox}
% Define output mode (default: six-page IJCAI)
\newtoggle{short}
\toggletrue{short}
% Uncomment for long version
\togglefalse{short}

\usepackage[scaled=0.9]{inconsolata}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}                    % For \vcentcolon
\usepackage{xspace}                       % Controls space after user-defined command
\usepackage{xparse}                       % Optional arguments in commands
\usepackage{enumitem}
\usepackage{fixltx2e}                     % For subscripts in normal text
\usepackage{bm}
\usepackage{dsfont}                       % Indicator function (mathbb for numbers)
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\newlength\figureheight
\newlength\figurewidth

\iftoggle{short}
{
\usepackage{nohyperref}
\usepackage{url}
}
{
\usepackage{flushend}
\usepackage[debug]{hyperref}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{%
  pdftitle={Active Learning for Level Set Estimation},
  pdfauthor={},
  pdfsubject={},
  pdfkeywords={},
  pdfborder=0 0 0,
  pdfpagemode=UseNone,
  colorlinks=true,
  linkcolor=mydarkblue,
  citecolor=mydarkblue,
  filecolor=mydarkblue,
  urlcolor=mydarkblue,
  pdfview=FitH}
}

\newcommand{\todo}[1]{\noindent\texttt{\small\color[rgb]{0.5,0.1,0.1} TODO: #1}}

% Refs
\newcommand{\sectref}[1]{\hyperref[#1]{Section \ref*{#1}}}
\newcommand{\chapref}[1]{\hyperref[#1]{Chapter \ref*{#1}}}
\newcommand{\figref}[1]{\hyperref[#1]{Figure \ref*{#1}}}
\newcommand{\figsref}[1]{\hyperref[#1]{Figures \ref*{#1}}}
\newcommand{\tabref}[1]{\hyperref[#1]{ Table \ref*{#1}}}
\newcommand{\algoref}[1]{\hyperref[#1]{Algorithm \ref*{#1}}}
\newcommand{\theoremref}[1]{\hyperref[#1]{Theorem \ref*{#1}}}
\newcommand{\lemmaref}[1]{\hyperref[#1]{Lemma \ref*{#1}}}
\newcommand{\lemmasref}[1]{\hyperref[#1]{Lemmas \ref*{#1}}}
\newcommand{\corref}[1]{\hyperref[#1]{Corollary \ref*{#1}}}
\newcommand{\asref}[1]{\hyperref[#1]{Assumption \ref*{#1}}}
\newcommand{\eqtref}[1]{\hyperref[#1]{\mbox{(\ref*{#1})}}}
\newcommand{\appref}[1]{\hyperref[#1]{Appendix \ref*{#1}}}
\newcommand{\lineref}[1]{\hyperref[#1]{line \ref*{#1}}}
\newcommand{\linesref}[2]{\hyperref[#1]{lines \ref*{#1}--\ref*{#2}}}
\newcommand{\linsref}[1]{\hyperref[#1]{lines \ref*{#1}}}

% Theory environments
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{assumption}{Assumption}

% Algorithm-related
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand*\LNot{\textbf{not}\xspace}
\newcommand*\LAnd{\textbf{and}\xspace}
\newcommand*\LET[2]{\STATE #1 $\gets$ #2}
\newcommand*\Fcall[1]{\textsc{#1}}
\makeatletter
\newcommand{\setalglineno}[1]{%
  \setcounter{ALC@line}{\numexpr#1-1}}
\makeatother
\renewcommand{\algorithmiccomment}[1]{// #1}
\newcommand{\LINEIF}[2]{%
    \STATE\algorithmicif\ {#1}\ \algorithmicthen\ {#2}%
}
\newcommand{\LINEELSE}[1]{%
    \STATE\algorithmicelse\ {#1}%
}

% Math definitions
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\def\*#1{\bm{#1}}

\newcommand{\twopartdef}[4]
{
	\left\{
		\begin{array}{ll}
			#1\,,& \mbox{if } #2 \\
			#3\,,& \mbox{if } #4
		\end{array}
	\right.
}

\newcommand{\twopartdefo}[3]
{
	\left\{
		\begin{array}{ll}
			#1\,,& \mbox{if } #2 \\
			#3\,,& \mbox{otherwise}
		\end{array}
	\right.
}

\newcommand{\defeq}{\vcentcolon=}

% Fix spacing problem with \left and \right
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

% Math definitions
\newcommand{\smid}{\ \middle\vert\ }
\newcommand{\mmid}{\,\vert\,}

\DeclareDocumentCommand \E { o m o }{%
  \IfValueTF {#1} {%
    \mathbb{E}_{#1}
  }{%
    \mathbb{E}
  }%
  \left[#2
  \IfValueT {#3} {%
    \smid #3
  }%
  \right]
}

\DeclareDocumentCommand \P { o m o }{%
  \IfValueTF {#1} {%
    \mathbb{P}_{#1}
  }{%
    \mathbb{P}
  }%
  \left[#2
  \IfValueT {#3} {%
    \smid #3
  }%
  \right]
}

\newcommand{\D}[2]{\Delta(#1\mmid#2)}
\newcommand{\sdef}[2]{\left\{#1\smid#2\right\}}

\newcommand{\pio}{\pi_{[0]}}
\newcommand{\pii}{\pi_{[i]}}
\newcommand{\pik}{\pi_{[k]}}
\newcommand{\pis}{\pi^*}
\newcommand{\pisi}{\pi^*_{[i]}}
\newcommand{\pisk}{\pi^*_{[k]}}
\newcommand{\pigr}{\pi^{\textrm{g}}}
\newcommand{\pig}{\pi^{\textrm{rg}}}
\newcommand{\pigo}{\pi^{\textrm{rg}}_{[0]}}
\newcommand{\pigi}{\pi^{\textrm{rg}}_{[i]}}
\newcommand{\pigl}{\pi^{\textrm{rg}}_{[\ell]}}
\newcommand{\pigii}{\pi^{\textrm{rg}}_{[i+1]}}
\newcommand{\pigk}{\pi^{\textrm{rg}}_{[k]}}
\newcommand{\favg}{f_{\mathrm{avg}}}
\newcommand{\dom}{\mathcal{D}}
\newcommand{\Mk}{\mathcal{M}_k}

\newcommand{\uitem}[1]{\item[#1]}

% For citations like: Author et. al [2010]
\newcommand{\citet}[1]{\citeauthor{#1}~\shortcite{#1}}

% Paragraph
\renewcommand{\paragraph}[1]{\vspace{0.3em}\noindent\textbf{#1.}\makebox[0.5em]{}}

% Shortcuts
\newcommand{\fbook}{\textsc{Facebook}}
\newcommand{\gnutella}{\textsc{Gnuttela}}
\newcommand{\gplus}{\textsc{GPlus}}
\newcommand{\twitter}{\textsc{Twitter}}
\newcommand{\infmax}{IM}
\newcommand{\maxcut}{MC}
%-------------------------------------------------------------------------------

\title{Non-monotone Adaptive Submodular Maximization}
\author{Alkis Gotovos\\
ETH Zurich
\And
Amin Karbasi\\
Yale University
\And
Andreas Krause\\
ETH Zurich}

\begin{document}

\maketitle

\begin{abstract}
A wide range of AI problems require sequentially selecting elements from a large set with the goal of optimizing some form of utility over the selected subset.
Moreover, each element that is picked may provide stochastic feedback, which can be used to make smarter decisions about future selections.
Finding efficient policies for this general class of adaptive optimization problems can be extremely hard.
However, when the objective function is adaptive monotone and adaptive submodular, a simple greedy policy attains a $1-1/e$ approximation ratio in terms of expected utility.
Unfortunately, many objectives of practical interest are naturally non-monotone; as a simple example, each selected element may incur some cost that needs to be subtracted from the original monotone utility, resulting in an non-monotone objective.
To our knowledge, no existing policy has provable performance guarantees when the assumption of adaptive monotonicity is lifted.
We propose the \emph{adaptive random greedy policy} for maximizing adaptive submodular functions, and prove that it retains the aforementioned $1-1/e$ approximation ratio for functions that are also adaptive monotone, while it additionally provides a $1/e$ ratio for non-monotone functions.
We showcase the benefits of our proposed policy on several network data sets using two non-monotone functions, representative of two classes of commonly encountered non-monotone objectives.
\end{abstract}

\section{Introduction}
Many practical problems in artificial intelligence boil down to selecting a number of elements from a large set of options in an initially unknown environment, so as to maximize some utility function defined over subsets of selected elements.
In contrast to the non-adaptive setting, where we commit to the selected elements all at once, in the adaptive setting the selection process is performed in a sequential manner, and each element that is picked provides some form of stochastic feedback, or, in other words, reveals part of the environment.
Naturally, we would like to leverage the acquired feedback to make smarter future selections.

As a running example, consider the toy instance of a stochastic maximum coverage problem shown in \figref{fig:toy}.
Suppose that triangle nodes represent birdwatching locations; by visiting each of them we observe a random number of bird species among those that are known to exist in that location.
We would like to plan a birdwatching trip that visits a number of these locations and maximizes the number of observed species.
It is intuitive that taking into account the alread observed species in making a decision about which place to visit next can be greatly beneficial compared to committing to the full trip in advance.

Since the general class of adaptive optimization problems in partially observable enviroments does not admit efficient policies, previous work has focused on characterizing subclasses of problems that can be solved efficiently.
Most notably, \citet{golovin11} showed that when the objective function under consideration is adaptive monotone and adaptive submodular, then a simple greedy policy attains a $1-1/e$-approximation in terms of expected utility.

In practice, while adaptive submodularity is an intuitive ``diminishing returns'' property for the adaptive setting, and is satisfied by a number of objective functions of interest, the additional assumption of adaptive monotonicity is often unrealistic.
In our birdwatching example, while the original objective is adaptive monotone submodular, if each subset of locations has an associated cost, e.g., the total required travel distance to visit all of them, then our new objective will be naturally non-monotone.
More generally, a common way of obtaining adaptive submodular objectives is by generalizing non-adaptive submodular functions under independence assumptions on the form of stochastic feedback of each observed element.
Despite the desire to generalize problems with non-monotone submodular objectives to the adaptive setting, no existing policies provide guarantees about non-monotone objectives.

In this paper, we propose the adaptive random greedy policy for maximizing non-monotone adaptive submodular functions.
We prove that this policy retains that $1-1/e$ approximation ratio, if the function is additionally adaptive monotone, and provides a $1/e$-approximation for non-monotone functions.
Our proposed policy generalizes the random greedy algorithm proposed by \citet{buchbinder14} for non-monotone submodular optimization to the adaptive setting.
We further discuss two common ways in which non-monotone adaptive submodular objectives come about, and present a representative example objective for each of them.
Finally, we evalute our policy on these two objectives using a number of real-world network data sets, and, thereby, showcase the potential benefits of adaptivity.

\section{Problem Statement and Background}
Assume we are given a finite ground set $E$ and a set $O$ of observable states.
Each item $e \in E$ is associated with a state $o \in O$ through a function $\phi : E \to O$, which is called a realization of the ground set.
In our setting, we assume that the realization $\Phi$ is a random variable with known distribution $p(\phi)$.
Furthermore, we are given an objective function $f : 2^E \times O^E \to \mathbb{R}_{\geq 0}$.
For a set $A \subseteq E$ and a realization $\phi$, the quantity $f(A, \phi)$ represents the utility of selecting subset $A$ when the true realization is $\phi$.

Our goal is to come up with a sequential policy that builds up a set $A \subseteq E$, such that our utility $f(A, \Phi)$ is maximized.
That is, we iteratively select an item $e \in E$ to add to $A$ and observe its state $\Phi(e)$.
In this setting, there are two factors that complicate matters compared to its non-adaptive counterpart.
First, since our utility depends on the (random) realization, we need to maximize the expected utility under the distribution of realizations $p(\phi)$.
Second, the chosen set $A$ itself is a random variable that depends on the realization, since the choices of our policy will change according to each observation $\Phi(e)$, which is, of course, the whole point of adaptivity.
In addition, the policy itself might make random decisions, which is an additional source of randomness for $A$.

To address the above complications, we define a partial realization as a set $ \psi \subseteq E \times O$, which represents the item-observation pairs over a subset of $E$.
In particular, we call this subset the \emph{domain} of $\psi$, which is formally defined as $\dom(\psi) \defeq \sdef{e \in E}{\exists o \in O : (e, o) \in \psi}$.
Additionally, we write $\psi(e) = o$, if $(e, o) \in \psi$, and call $\psi$ \emph{consistent} with realization $\phi$ (denoted by $\phi \sim \psi$), if $\psi(e) = \phi(e)$, for all $e \in \dom(\psi)$, which means that the observations of a subset according to $\psi$ agree with the assignments over the whole ground set according to $\phi$.

Now, we can define a \emph{policy} $\pi$ as a function from partial realizations to a distribution over which item to pick next, formally, $\pi : 2^{E \times O} \to \mathcal{P}(E)$.
The policy terminates when the current partial realization is not in its domain denoted by $\dom(\pi) \subseteq 2^{E \times O}$.
We use the notation $\pi(e\mmid\psi)$ for the probability of picking item $e$ given partial realization $\psi$.
We call $E(\pi, \Phi) \subseteq E$ the set of items that have been selected upon termination of policy $\pi$ under realization $\Phi$.
Note that this set a random variable that depends on both the randomness of the policy, as well as the randomness of the realization.

Finally, we can formally assess the performance of a policy $\pi$ via its expected utility,
\begin{align*}
  \favg(\pi) \defeq \E[\Phi,\Pi]{f(E(\pi, \Phi), \Phi)}.
\end{align*}
Then, our goal is to come up with a policy that maximizes the expected utility, subject to a cardinality constraint on the number of items to be picked, $|E(\pi, \Phi)| \leq k$.

\subsection{Monotonicity and Submodularity}
\paragraph{Non-adaptive}
Even in the non-adaptive setting, where we have to commit to selecting a subset without any feedback, the problem of maximizing the values of a set function $f : 2^E \to \mathbb{R}_{\geq 0}$, subject to a cardinality constraint $|A| \leq k$, is NP-hard in general.
In this setting, the marginal gain of an element $e \in E$ given set $B \subseteq E$ is defined as $f(B \cup \{e\}) - f(B)$.
Intuitively, the marginal gain quantifies the increase in utility if we add $e$ to our selection, given that we have already picked the elements in $B$.
Function $f$ is called monotone if, for any $B \subseteq C \subseteq E$, it holds that $f(B) \leq f(C)$, which is equivalent to saying that the marginal gain is always positive.
Furthermore, $f$ is called submodular, if for any $B \subseteq C \subseteq E$ and any $e \in E \setminus C$, it holds that $f(C \cup \{e\}) - f(C) \leq  f(B \cup \{e\}) - f(B)$.
This means that the marginal gain of any element decreases as the given set increases ($C \supseteq B$); in other words, it expresses a behavior of ``diminishing returns''.

In their famous result, \citet{nemhauser78} showed that, if $f$ is non-negative, monotone, and submodular, then constructing a subset $A$ by picking elements greedily according to their marginal gains, guarantees that $f(A)$ is a ($1 - 1/e$)-approximation to the optimal value.

\paragraph{Adaptive}
In the significantly more complex adaptive setting, the problem of computing an optimal policy is hard to approximate even for seemingly simple classes of objective functions (e.g., linear) \cite{golovin11}.
However, they also showed that the notions of monotonicity and submodularity can be naturally generalized to this setting, and be used to give similar performance guarantees as in the non-adaptive case.

More concretely, the expected marginal gain of element $e \in E$ given partial realization $\psi$ can be defined as
\begin{align*}
  \D{e}{\psi} \defeq \E[\Phi]{f\big(\mathcal{D}(\psi) \cup \{e\}, \Phi\big) - f\big(\dom(\psi), \Phi\big)}[\Phi \sim \psi].
\end{align*}
The above expression is a conditional expectation, which only considers realizations that are consistent with $\psi$.
Then, the following properties can be defined analogously to their non-adaptive counterparts:
\begin{itemize}
\item $f$ is called adaptive monotone, if $\D{e}{\psi} \geq 0$, for all $e \in E$ and all $\psi$ of positive probability,
\item $f$ is called adaptive submodular, if $\D{e}{\psi'} \leq \D{e}{\psi}$, for all $e \in E \setminus \dom(\psi')$ and all $\psi' \supseteq \psi$.
\end{itemize}
A stronger condition, which is often much easier to verify in practice, is that $f(\cdot, \phi)$ be submodular for any $\phi$, which immediately implies adaptive submodularity of $f$ for any distribution $p(\phi)$.

Given a number of previously selected elements and their corresponding observed states encoded in a partial realization $\psi$, the adaptive greedy policy $\pigr$ selects the element $e \in E \setminus \psi$ that achieves the highest marginal gain $\D{e}{\psi}$, and does so iteratively until $k$ elements have been selected.
\citet{golovin11} showed that under the assumption of adaptive monotonicity and submodularity, $\pigr$ is a $(1-1/e)$-approximation to the optimal policy in terms of expected utility $\favg$.

\section{Adaptive Random Greedy}
While many functions of interest have been shown to satisfy adaptive monotonicity, there are naturally occuring non-monotone functions, for which there is no existing policy with provable performance gurantees.
We now present our proposed adaptive random greedy policy ($\pig$) for maximizing adaptive submodular functions, and prove approximation ratios for both adaptive monotone and non-monotone objectives.

For technical reasons that will become apparent below, let us assume that we always add a set $D$ of $2k - 1$ dummy elements to the ground set, such that, for any $d \in D$, and any partial realization $\psi$, it holds that $\D{d}{\psi} = 0$.
Obviously, these elements do not affect the optimal policy, and may be removed from the solution of any policy, without affecting its expected utility.

The detailed pseudocode of using adaptive random greedy is presented \algoref{alg:rg}.
As discussed before, we are given a ground set and an objective function, as well as a known distribution over realizations $\Phi$.
After computing the marginal gains
The key difference compared to the adaptive greedy policy is shown in \linesref{lin:upd1}{lin:upd2} of the algorithm.
Rather than selecting the element with the largest expected marginal gain, $\pig$ randomly selects an element among the ones with the $k$ largest gains.
The dummy elements added to the ground set ensure that the policy never picks an element with negative expected marginal gain.
Also, note that, although in \algoref{alg:rg} the returned set $A$ contains exactly $k$ elements, since we remove any dummy elements from it, the actual selected set may very well contain less than $k$ elements.

When running the original adaptive greedy policy, non-monotonicity can lead to situations, where selecting the element of maximum marginal gain leads to traps of low utility that cannot be escaped by that policy.
In contrast, the randomness introduced by adaptive random greedy to the selection of each element helps dealing with such traps (on average) and, thus, leads to guaranteed approximation guarantees for the expected utility, even for non-monotone objectives.

\begin{algorithm}[tb]
  \caption{Adaptive random greedy}
  \label{alg:rg}
  \normalsize{
    \begin{algorithmic}[1]
      \REQUIRE ground set $E$, function $f$, distribution $p(\phi)$, cardinality constraint $k$
      %  \ENSURE set $S_k \subseteq E$
      \LET{$A$}{$\varnothing$}
      \LET{$\psi$}{$\varnothing$}
      \FOR{$i = 1$ \TO $k$}
      \STATE Compute $\D{e}{\psi}$, for all $e \in E$ \label{lin:marg}
      \LET{$\Mk(\psi)$}{$\displaystyle\argmax_{S \subseteq E \setminus A,\,|S| = k}\left\{\sum_{e \in S} \D{e}{\psi} \right\}$} \label{lin:upd1}
      \STATE Sample $m$ uniformly at random from $\Mk(\psi)$ \label{lin:upd2}
      \LET{$A$}{$A \cup \{m\}$}
      \STATE Observe $\Phi(m)$
      \LET{$\psi$}{$\psi \cup \left\{\big(m, \Phi(m)\big)\right\}$}
      \ENDFOR
      \STATE Return $A$
    \end{algorithmic}
  }
\end{algorithm}

\paragraph{Theoretical analysis}
More concretely, we now show that the adaptive random greedy policy retains the $1-1/e$ approximation ratio for adaptive monotone submodular objectives, while, at the same time, it achieves a $1/e$ approximation ratio for non-monotone objectives, under the somewhat stronger condition of submodularity for each realization.
As stated in the previous section, this stronger condition implies adaptive submodularity and, in fact, is satisfied in the majority of practical applications, since it is the most common way to prove adaptive submodularity in the first place.
Therefore, we do not consider it a major restriction in the choice of objective functions.
Our proofs generalize the results of \cite{buchbinder14} for the random greedy algorithm and are present in detail in the longer version of this paper; in what follows, we provide an outline of our analysis.

First, let us define the expected gain of running policy $\pi$ after having obtained a partial realization $\psi$, as
{\small
\begin{align*}
  \D{\pi}{\psi} \defeq \E[\Phi,\Pi]{f(\dom(\psi) \cup E(\pi, \Phi), \Phi) - f(\dom(\psi), \Phi)}[\Phi \sim \psi].
\end{align*}}
Also, for any policy $\pi$, we define a truncated policy $\pik$, which runs indentically to $\pi$ for $k$ steps and then terminates.
Finally, given any two policies $\pi_1$, $\pi_2$, we denote by $\pi_1@\pi_2$ a new policy that first runs $\pi_1$ until it terminates and then continues with $\pi_2$, discarding the observations made by $\pi_1$.
In the end, the subset selected by $\pi_1@\pi_2$ will be the union of the subsets selected by each policy individually.

The following is a key lemma for both monotone and non-monotone objectives.
\setcounter{lemma}{0}
\begin{lemma}
  If $f$ is adaptive submodular, then, for any policy $\pi$, and any partial realization $\psi$, it holds that
  \begin{align*}
    \D{\pik}{\psi} \leq \sum_{e \in \Mk(\psi)} \D{e}{\psi}.
  \end{align*}
\end{lemma}
\noindent It states the intuitive fact that, at any point, no $k$-step policy can give us a larger expected gain than the sum of the $k$ currently largest expected marginal gains.
This is a consequence of adaptive submodularity, which guaratees that the expected marginal gains of any element will decrease as our selection grows larger.

Based on \lemmaref{lem:submod} and the fact that $\pig$ selects at each step one of the $k$ elements in $\Mk$ uniformly at random, we can show the following lemma, which also applies to both monotone and non-monotone objectives.
\begin{lemma}\label{lem:mon_main}
  For any policy $\pi$ and any non-negative integer $i < k$, if $f$ is adaptive submodular, then
  \begin{align*}
    \favg(\pigii) - \favg(\pigi) \geq \frac{1}{k}\left(\favg(\pigi @ \pi) - \favg(\pigi)\right).
  \end{align*}
\end{lemma}
\noindent The lemma compares the expected gain at the $i$-th step of $\pig$ to the total gain of running any other policy (e.g., the optimal one) after the $i$-th step and, thereby, provides a means for obtaining approximation guarantees for $\pig$, as long as we can bound the term $\favg(\pigi @ \pi)$.

If $f$ is adaptive monotone, we may use the bound $\favg(\pigi @ \pi) \geq \favg(\pi)$, which readily follows from the definition of adaptive monotonicity, to obtain the following theorem.
\begin{theorem}
  If $f$ is adaptive monotone submodular, then for any policy $\pi$ and all integers $i, k > 0$ it holds that
  \begin{align*}
    \favg(\pigi) \geq \left(1 - e^{-i/k}\right)\favg(\pik).
  \end{align*}
\end{theorem}
\noindent In particular, by setting $i = k$ we get the familiar $1-1/e$ approximation ratio for $\pigk$.

For the non-monotone case, we need to leverage the randomness of the selection process of $\pig$ to bound $\favg(\pigi @ \pi)$.
For that purpose, we generalize to the adaptive setting the following lemma shown by \citet{buchbinder14}, which itself is based on a lemma by \citet{feige07} for the expected value of a submodular function under a randomly selected subset.
\begin{lemma}[\cite{buchbinder14}]
  If $f : 2^E \to \mathbb{R}_{\geq 0}$ is submodular and $A$ is a random subset of $E$, such that each element $e \in E$ is contained in $A$ with probability at most $p$, that is, $\P[A]{e \in A} \leq p,\ \forall e \in E$, then
  \begin{align*}
    \E[A]{f(A)} \geq (1-p)f(\varnothing).
  \end{align*}
\end{lemma}
\noindent Roughly speaking, the lemma states that a ``random enough'' subset $A$ cannot have much worse value than that of the empty set.
Note that $f$ here is not assumed to be monotone.

The following lemma extends the above claim to the adaptive setting.
\begin{lemma}
  If $f(\cdot\,, \phi) : 2^E \to \mathbb{R}_{\geq 0}$ is submodular for all $\phi \in O^E$, then for any policy $\pi$ such that each element of $e \in E$ is selected by it with probability at most $p$, that is, $\P[\Pi]{e \in E(\pi, \phi)} \leq p,\ \forall \phi \in O^E,\ \forall e \in E$, the expected value of running $\pi$ can be bounded as follows:
\begin{align*}
  \favg(\pi) \geq (1-p)\,\favg(\pio).
\end{align*}
\end{lemma}
\noindent Although we have not found concrete examples of adaptive submodular functions that do not satisfy this lemma, adaptive submodularity by itself does not seem to be a sufficient condition for it to hold; rather, we require the stronger assumption that $f$ be submodular for any realization.

As a consequence of the above lemma, we get that $\favg(\pigi@\pi) = \favg(\pi@\pigi) \geq (1-p)\favg(\pi)$, meaning that the elements added by adaptive random greedy cannot reduce the average value obtained by any other policy $\pi$ by too much.
The probability $p$ in this case can be bounded by using the fact that $\pig$ randomly selects one of $k$ elements at each step, therefore at the $i$-step we have $p \leq (1-1/k)^i$.
Putting it all together, we obtain the following theorem for non-monotone objectives.
\begin{theorem}\label{thm:nonm}
  If $f(\cdot\,, \phi) : 2^E \to \mathbb{R}_{\geq 0}$ is submodular for all $\phi \in O^E$, then, for any policy $\pi$, and all integers $i, k > 0$, it holds that
  \begin{align*}
    \favg(\pigi) \geq \frac{i}{k}\left(1 - \frac{1}{k}\right)^{i-1}\favg(\pik).
  \end{align*}
\end{theorem}
\noindent By setting $i = k$ we get a $1/e$ approximation ratio for $\pigk$.

\section{Examples of Non-Monotone Objectives}
To underline the importance of non-monotonicity, we now present two different ways, in which non-monotone adaptive submodular functions commonly arise in practice.
The two resulting classes of functions satisfy the assumptions of \theoremref{thm:nonm}, and are, therefore, suitable to be maximized using adaptive random greedy.
We also introduce two representative example objectives, one for each class, which are themselves of practical interest.

\subsection{Objectives with a Modular Cost Term}
Assume we are given an adaptive monotone submodular function $f_{\textrm{utility}}(A, \phi)$.
In practice, apart from benefit, there might also be some associated cost with the selection of each element, which can be directly incorporated into the objective function via a modular cost term a cost term $f_{\textrm{cost}}(A) = \sum_{a \in A} c_a$.
In this case, the resulting objective is of the form
\begin{align} \label{eq:cost}
  f(A, \phi) = f_{\textrm{utility}}(A, \phi) - f_{\textrm{cost}}(A),
\end{align}
which is also adaptive submodular, but non-monotone.

\paragraph{Influence maximization}
The concept of influence maximization in a social network was posed by \citet{kempe03} and has direct applications to problems such as viral marketing.
Given a graph and a model of influence propagation, the goal is to select a subset of nodes that are initially active, in order to maximize the spread of influence measured by the expected number of nodes that will ultimately be active according to the propagation model.
We focus here on the independent cascade model, according to which, each edge of the graph is randomly either ``live'' or ``blocked'', independently of any other edge in the network, and influence can only flow along ``live'' edges.

For this problem, the ground set $E$ consists of network nodes and each realization $\Phi$ corresponds to a full outcome of the independent cascade model, that is, a boolean assignment to each network edge of being either ``live'' or ``blocked''.
The objective function $f_{\textrm{inf}}(A, \Phi)$ is the number of ultimately active nodes under realization $\Phi$, if the nodes in the selected subset $A$ are initially active.
In the adaptive version of the problem, when a node $v$ is selected, it reveals the status (``live'' or ``blocked'') of all outgoing edges of $v$ and of any other node that can be reached from $v$ through ``live'' edges.
\citet{kempe03} showed that $f_{\textrm{inf}}$ is non-negative, monotone, and submodular, for any realization $\Phi$, and \citet{golovin11} showed that it is also adaptive monotone submodular.

Now, assume that each selected node incurs a unit cost, that is, we have a cost term $f_{\textrm{cost}}(A) = |A|$, which results in the following objective:
\begin{align*}
  \tilde{f}_{\textrm{inf}}(A, \Phi) \defeq f_{\textrm{inf}}(A, \Phi) - |A|.
\end{align*}
Since our cost term is modular, $\tilde{f}_{\textrm{inf}}$ has the form of equation \eqref{eq:cost}, and, therefore, is non-monotone adaptive submodular.
It is also non-negative, since $f_{\textrm{inf}}(A, \Phi) \geq |A|$.

\setlength\figureheight{0.25\textwidth}
\setlength\figurewidth{0.29\textwidth}
\newcommand{\subflen}{0.245\textwidth}
\newcommand{\scspacey}{-1.5em}
\newcommand{\scspacex}{0.5em}
\begin{figure*}[tb]
  \captionsetup[subfigure]{oneside,margin={3em,0em}}
  \begin{subfigure}[b]{\subflen}
    \centering
    \input{figures/inf_ego_fb.tex}
    \vspace{\scspacey}
    \caption{\hspace{\scspacex}\infmax -- \fbook}
    \label{fig:inf_ego_fb}
  \end{subfigure}
  \begin{subfigure}[b]{\subflen}
    \input{figures/inf_gnutella.tex}
    \vspace{\scspacey}
    \caption{\hspace{\scspacex}\infmax -- \gnutella}
    \label{fig:inf_gnutella}
  \end{subfigure}
  \begin{subfigure}[b]{\subflen}
    \input{figures/inf_gplus.tex}
    \vspace{\scspacey}
    \caption{\hspace{\scspacex}\infmax -- \gplus}
    \label{fig:inf_gplus}
  \end{subfigure}
  \begin{subfigure}[b]{\subflen}
    \input{figures/inf_twitter.tex}
    \vspace{\scspacey}
    \caption{\hspace{\scspacex}\infmax -- \twitter}
    \label{fig:inf_twitter}
  \end{subfigure}\\[0.5em]
  \begin{subfigure}[b]{\subflen}
    \input{figures/mc_ego_fb.tex}
    \vspace{\scspacey}
    \caption{\hspace{\scspacex}\maxcut -- \fbook}
    \label{fig:mc_ego_fb}
  \end{subfigure}
  \begin{subfigure}[b]{\subflen}
    \input{figures/mc_gnutella.tex}
    \vspace{\scspacey}
    \caption{\hspace{\scspacex}\maxcut -- \gnutella}
    \label{fig:mc_gnutella}
  \end{subfigure}
  \begin{subfigure}[b]{\subflen}
    \input{figures/mc_gplus.tex}
    \vspace{\scspacey}
    \caption{\hspace{\scspacex}\maxcut -- \gplus}
    \label{fig:mc_gplus}
  \end{subfigure}
  \begin{subfigure}[b]{\subflen}
    \input{figures/mc_twitter.tex}
    \vspace{\scspacey}
    \caption{\hspace{\scspacex}\maxcut -- \twitter}
    \label{fig:mc_twitter}
  \end{subfigure}
  \caption{
    \todo{}
  }
  \label{fig:exp}
\end{figure*}

\subsection{Objectives with Factorial Realizations}
Assume we are given a utility function $f(A, \phi)$, whose dependence of $\phi$ is constrained to the outcomes of the selected elements.
More formally, there exists a function $g : 2^{E \times O} \to \mathbb{R}_{\geq 0}$, such that $f(A, \phi) = g\left(\{(e, \phi(e)) \mid e \in A\}\right)$.
Furthermore, assume that $f$ is submodular in its first argument, for any realization $\phi$, and that the distribution of realizations factorizes over the elements of the ground set $E$, that is, $\P[\Phi]{\phi} = \prod_{e \in E}\P[\Phi(e)]{\phi(e)}$.
Given the above assumptions, it follows that $f$ is adaptive submodular (cf. Theorem 6.1 of \citet{golovin11}).

\paragraph{Maximum graph cut}
The problem of finding the maximum cut in a graph $(\mathcal{V}, \mathcal{E})$ can be posed using the following objective:
\begin{align*}
  f_{\textrm{cut}}(A) = \sum_{(v, w) \in \mathcal{E}} \mathds{1}_{\left\{v \in A, w \in \mathcal{V}\setminus A\ \textrm{or}\ v \in \mathcal{V}\setminus A, w \in A\right\}},
\end{align*}
which is non-negative and submodular.
Furthermore, it is symmetric, i.e., $f(A) = f(\mathcal{V}\setminus A)$, which implies that it is also non-monotone.
We consider here an adaptive version of the maximum cut problem, in which, the selection of a node triggers either cutting that node itself, or a random neighbor of it with some prespecified probability.

Again, the ground set $E$ consists of network nodes, and each realization $\phi$ corresponds to a function $\sigma_{\phi}$ that maps each node $v \in V$ to the node that would actually be cut, were $v$ to be selected.
Our adaptive max-cut objective can then be written as follows:
\begin{align*}
  \tilde{f}_{\textrm{cut}}(A, \Phi) \defeq f_{\textrm{cut}}\left(\bigcup_{v \in A}\sigma_{\Phi}(v)\right).
\end{align*}
We can directly see that, in terms of the realization $\Phi$, $\tilde{f}_{\textrm{cut}}$ only depends on the outcomes of the selected elements.
Furthermore, the distribution of realizations is factorial, since the outcome of each node cut is independent of all the others, i.e., $\sigma_{\phi}(v) = \hat{\sigma}_{\phi(v)}(v)$.
Finally, since $f_{\textrm{cut}}$ is submodular, it follows that $\tilde{f}_{\textrm{cut}}$ is submodular in its first argument for any realization.
We conclude that $\tilde{f}_{\textrm{cut}}$ satisfies the properties described above, hence it is (non-monotone) adaptive submodular.
It is also non-negative, because $f_{\textrm{cut}}$ is non-negative.

\section{Experiments}
We have evaluated our proposed algorithm on the two objective functions, namely influence maximization, and maximum cut, described in the previous section on several real-world data sets.
Since we have no competitor policy for the adaptive non-monotone submodular setting, we rather focus here on showcasing the potential benefits of adaptivity by comparing adaptive to non-adaptive random greedy on these two objectives.

\subsection{Data Sets and Experimental Setup}
For our experiments, we used $20$ networks from the KONECT\footnote{\url{http://konect.uni-koblenz.de/}} database, which accumulates network data sets from various other sources.
The network sizes range from a few thousands to tens of thousands nodes.

Computing the (expected) marginal gains is at the heart of both the non-adaptive random greedy algorithm and our proposed random greedy policy.
This computation may range from being completely straightforward to extremely demanding, depending on the specific objective at hand.
For the influence optimization objective, the exact computation of the expected influence of a subset of nodes has been shown to be NP-hard \cite{kempe03}.
To obtain an estimate we use Monte Carlo sampling over the outcomes of the independent cascades.
Note within adaptive random greedy we have to perform this simulation at every step conditioning on the observations obtained up to that step.
For the maximum cut objective, it is very simple to compute the marginal gains in the adaptive case, since we already know at each step the outcomes of the previously selected nodes.
In the non-adaptive setting, however, this is considerably harder, since we have to average over every possible outcome of the current selection; we again resort to sampling from the space of possible realizations to obtain estimates.
To make the aforementioned computations more efficient, we subsample each network down to $2000$ nodes, using a technique based on random walks proposed by \citet{leskovec06}.

For both objectives, we select uniformly at random a subset of \todo{$50$} nodes as the ground set $E$, and repeat the experiments for \todo{$100$} such random ground sets.
For each ground set instance, we evaluate the algorithms' performance on \todo{$100$} random realizations.

\subsection{Results}

\section{Related Work}
Compared to monotone submodular maximization, for which the $(1-1/e)$-approximation of the greedy algorithm was shown by \citet{nemhauser78}, constant-factor approximations for non-monotone submodular functions have been much more recent, for both the unconstrained case \cite{feige07}, as well as under matroid and knapsack constraints \cite{lee09}.
Even more recently, \citet{buchbinder14} introduced the random greedy algorithm for maximizing non-monotone submodular functions under a cardinality constraint, from which we drew inspiration for our proposed adaptive random greedy policy.

The concepts of adaptive monotonicity and adaptive submodularity were introduced by \citet{golovin11}, who also showed that the greedy policy provides a $(1-1/e)$-approximation under these assumptions.
Example application domains, apart from those we present in this paper, include active learning \cite{chen14} and incentive mechanism design \cite{singla13}.

The problem of influence maximization was originally proposed by \citet{kempe03} and was extended to the adaptive setting by \citet{golovin11}.
Various techniques have been proposed to make the computation of marginal gains feasible for large-scale networks using, for instance, more efficient sampling methods \cite{ohsaka14}, and sketching-based approximations \cite{cohen14}.
In this paper we chose to run experiments on smaller-scale networks, but these techniques could also be applied to scale up adaptive random greedy as well.
\citet{he14} recently considered the problem of assessing the robustness of influence maximization algorithms under network parameter misspecification, which interestingly leads to maximizing a non-monotone submodular objective.

Maximum cut has been a much-studied NP-complete problem with provable approximation guarantees given by SDP-based algorithms for both the unconstrained \cite{goemans95} and cardinality-constrained \cite{feige01} cases.
An interesting application of maximum cut objectives has been proposed by \citet{lin10} and \citet{lin11} for text summarization.

\section{Conclusion}

\clearpage
% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai15}

\iftoggle{short}
{}
{
\clearpage
\onecolumn
\appendix
\setcounter{lemma}{0}
\setcounter{theorem}{0}
\include{proofs}
}

\end{document}
