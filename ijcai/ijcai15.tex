%%%% ijcai15.tex

\typeout{Non-monotone Adaptive Submodular Maximization}

% These are the instructions for authors for IJCAI-15.
% They are the same as the ones for IJCAI-11 with superficical wording
%   changes only.

\documentclass{article}
% The file ijcai15.sty is the style file for IJCAI-15 (same as ijcai07.sty).
\usepackage{ijcai15}

% Use the postscript times font!
\usepackage{times}

% the following package is optional:
%\usepackage{latexsym} 

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

%-------------------------------------------------------------------------------
% Custom definitions
%-------------------------------------------------------------------------------
\usepackage{etoolbox}
% Define output mode (default: six-page IJCAI)
\newtoggle{short}
\toggletrue{short}
% Uncomment for long version
\togglefalse{short}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}                    % For \vcentcolon
\usepackage{xspace}                       % Controls space after user-defined command
\usepackage{xparse}                       % Optional arguments in commands
\usepackage{enumitem}
\usepackage{fixltx2e}                     % For subscripts in normal text
\usepackage{bm}
\usepackage{dsfont}                       % Indicator function (mathbb for numbers)
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pgfplots}
\iftoggle{short}
{
\usepackage{nohyperref}
\usepackage{url}
}
{
\usepackage{flushend}
\usepackage[debug]{hyperref}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{%
  pdftitle={Active Learning for Level Set Estimation},
  pdfauthor={},
  pdfsubject={},
  pdfkeywords={},
  pdfborder=0 0 0,
  pdfpagemode=UseNone,
  colorlinks=true,
  linkcolor=mydarkblue,
  citecolor=mydarkblue,
  filecolor=mydarkblue,
  urlcolor=mydarkblue,
  pdfview=FitH}
}

\newcommand{\todo}[1]{\noindent\texttt{\small\color[rgb]{0.5,0.1,0.1} TODO: #1}}

% Refs
\newcommand{\sectref}[1]{\hyperref[#1]{Section \ref*{#1}}}
\newcommand{\chapref}[1]{\hyperref[#1]{Chapter \ref*{#1}}}
\newcommand{\figref}[1]{\hyperref[#1]{Figure \ref*{#1}}}
\newcommand{\figsref}[1]{\hyperref[#1]{Figures \ref*{#1}}}
\newcommand{\tabref}[1]{\hyperref[#1]{ Table \ref*{#1}}}
\newcommand{\algoref}[1]{\hyperref[#1]{Algorithm \ref*{#1}}}
\newcommand{\theoremref}[1]{\hyperref[#1]{Theorem \ref*{#1}}}
\newcommand{\lemmaref}[1]{\hyperref[#1]{Lemma \ref*{#1}}}
\newcommand{\lemmasref}[1]{\hyperref[#1]{Lemmas \ref*{#1}}}
\newcommand{\corref}[1]{\hyperref[#1]{Corollary \ref*{#1}}}
\newcommand{\asref}[1]{\hyperref[#1]{Assumption \ref*{#1}}}
\newcommand{\eqtref}[1]{\hyperref[#1]{\mbox{(\ref*{#1})}}}
\newcommand{\appref}[1]{\hyperref[#1]{Appendix \ref*{#1}}}
\newcommand{\lineref}[1]{\hyperref[#1]{line \ref*{#1}}}
\newcommand{\linesref}[2]{\hyperref[#1]{lines \ref*{#1}--\ref*{#2}}}
\newcommand{\linsref}[1]{\hyperref[#1]{lines \ref*{#1}}}

% Theory environments
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{assumption}{Assumption}

% Algorithm-related
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand*\LNot{\textbf{not}\xspace}
\newcommand*\LAnd{\textbf{and}\xspace}
\newcommand*\LET[2]{\STATE #1 $\gets$ #2}
\newcommand*\Fcall[1]{\textsc{#1}}
\makeatletter
\newcommand{\setalglineno}[1]{%
  \setcounter{ALC@line}{\numexpr#1-1}}
\makeatother
\renewcommand{\algorithmiccomment}[1]{// #1}
\newcommand{\LINEIF}[2]{%
    \STATE\algorithmicif\ {#1}\ \algorithmicthen\ {#2}%
}
\newcommand{\LINEELSE}[1]{%
    \STATE\algorithmicelse\ {#1}%
}

% Math definitions
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\def\*#1{\bm{#1}}

\newcommand{\twopartdef}[4]
{
	\left\{
		\begin{array}{ll}
			#1\,,& \mbox{if } #2 \\
			#3\,,& \mbox{if } #4
		\end{array}
	\right.
}

\newcommand{\twopartdefo}[3]
{
	\left\{
		\begin{array}{ll}
			#1\,,& \mbox{if } #2 \\
			#3\,,& \mbox{otherwise}
		\end{array}
	\right.
}

\newcommand{\defeq}{\vcentcolon=}

% Fix spacing problem with \left and \right
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

% Math definitions
\newcommand{\smid}{\ \middle\vert\ }
\newcommand{\mmid}{\,\vert\,}

\DeclareDocumentCommand \E { o m o }{%
  \IfValueTF {#1} {%
    \mathbb{E}_{#1}
  }{%
    \mathbb{E}
  }%
  \left[#2
  \IfValueT {#3} {%
    \smid #3
  }%
  \right]
}

\DeclareDocumentCommand \P { o m o }{%
  \IfValueTF {#1} {%
    \mathbb{P}_{#1}
  }{%
    \mathbb{P}
  }%
  \left[#2
  \IfValueT {#3} {%
    \smid #3
  }%
  \right]
}

\newcommand{\D}[2]{\Delta(#1\mmid#2)}
\newcommand{\sdef}[2]{\left\{#1\smid#2\right\}}

\newcommand{\pio}{\pi_{[0]}}
\newcommand{\pii}{\pi_{[i]}}
\newcommand{\pik}{\pi_{[k]}}
\newcommand{\pis}{\pi^*}
\newcommand{\pisi}{\pi^*_{[i]}}
\newcommand{\pisk}{\pi^*_{[k]}}
\newcommand{\pigr}{\pi^{\textrm{g}}}
\newcommand{\pig}{\pi^{\textrm{rg}}}
\newcommand{\pigo}{\pi^{\textrm{rg}}_{[0]}}
\newcommand{\pigi}{\pi^{\textrm{rg}}_{[i]}}
\newcommand{\pigl}{\pi^{\textrm{rg}}_{[\ell]}}
\newcommand{\pigii}{\pi^{\textrm{rg}}_{[i+1]}}
\newcommand{\pigk}{\pi^{\textrm{rg}}_{[k]}}
\newcommand{\favg}{f_{\mathrm{avg}}}
\newcommand{\dom}{\mathcal{D}}
\newcommand{\Mk}{\mathcal{M}_k}

\newcommand{\uitem}[1]{\item[#1]}

% For citations like: Author et. al [2010]
\newcommand{\citet}[1]{\citeauthor{#1}~\shortcite{#1}}

% Paragraph
\renewcommand{\paragraph}[1]{\vspace{0.3em}\noindent\textbf{#1.}\makebox[0.5em]{}}
%-------------------------------------------------------------------------------

\title{Non-monotone Adaptive Submodular Maximization}
\author{Alkis Gotovos\\
ETH Zurich
\And
Amin Karbasi\\
Yale University
\And
Andreas Krause\\
ETH Zurich}

\begin{document}

\maketitle

\begin{abstract}
A wide range of AI problems require us to sequentially select elements from a large set with the goal of optimizing some form of utility of our selected subset.
Moreover, each element we pick may provide us with stochastic feedback, which we can use to make smarter decisions about future selections.
While finding efficient policies for this general class of adaptive stochastic optimization problems can be extremely hard, the subclass of problems, for which the objective function is adaptive monotone and adaptive submodular, has been shown to possess a number of desirable properties; most notably, the simple and efficient greedy policy obtains a $1-1/e$ approximation ratio.
However, many objectives of practical interest are naturally non-monotone; as a simple example, each selected element may incur some cost that needs to be subtracted from the original monotone utility, resulting in a non-monotone objective.
For these objectives, there are no existing policies with provable performance guarantees.
We propose the \emph{random greedy policy} for maximizing adaptive submodular functions, and prove that it retains the aforementioned $1-1/e$ approximation ratio for functions that are also adaptive monotone, while it additionally provides a $1/e$ ratio for non-monotone functions.
We showcase the benefits of our proposed policy on several small-scale network data sets using two non-monotone objectives.
\end{abstract}

\section{Introduction}

\clearpage
\section{Problem Statement and Background}
Assume we are given a finite ground set $E$ and a set $O$ of observable states.
Each item $e \in E$ is associated with a state $o \in O$ through a function $\phi : E \to O$, which is called a realization of the ground set.
In our setting, we assume that the realization $\Phi$ is a random variable with known distribution $p(\phi)$.
Furthermore, we are given an objective function $f : 2^E \times O^E \to \mathbb{R}_{\geq 0}$.
For a set $A \subseteq E$ and a realization $\phi$, the quantity $f(A, \phi)$ represents the utility of selecting subset $A$ when the true realization is $\phi$.

Our goal is to come up with a sequential policy that builds up a set $A \subseteq E$, such that our utility $f(A, \Phi)$ is maximized.
That is, we iteratively select an item $e \in E$ to add to $A$ and observe its state $\Phi(e)$.
In this setting, there are two factors that complicate matters compared to its non-adaptive counterpart.
First, since our utility depends on the (random) realization, we need to maximize the expected utility under the distribution of realizations $p(\phi)$.
Second, the chosen set $A$ itself is a random variable that depends on the realization, since the choices of our policy will change according to each observation $\Phi(e)$, which is, of course, the whole point of adaptivity.
In addition, the policy itself might make random decisions, which is an additional source of randomness for $A$.

To address the above complications, we define a partial realization as a set $ \psi \subseteq E \times O$, which represents the item-observation pairs over a subset of $E$.
In particular, we call this subset the \emph{domain} of $\psi$, which is formally defined as $\dom(\psi) \defeq \sdef{e \in E}{\exists o \in O : (e, o) \in \psi}$.
Additionally, we write $\psi(e) = o$, if $(e, o) \in \psi$, and call $\psi$ \emph{consistent} with realization $\phi$ (denoted by $\phi \sim \psi$), if $\psi(e) = \phi(e)$, for all $e \in \dom(\psi)$, which means that the observations of a subset according to $\psi$ agree with the assignments over the whole ground set according to $\phi$.

Now, we can define a \emph{policy} $\pi$ as a function from partial realizations to a distribution over which item to pick next, formally, $\pi : 2^{E \times O} \to \mathcal{P}(E)$.
The policy terminates when the current partial realization is not in its domain denoted by $\dom(\pi) \subseteq 2^{E \times O}$.
We use the notation $\pi(e\mmid\psi)$ for the probability of picking item $e$ given partial realization $\psi$.
We call $E(\pi, \Phi) \subseteq E$ the set of items that have been selected upon termination of policy $\pi$ under realization $\Phi$.
Note that this set a random variable that depends on both the randomness of the policy, as well as the randomness of the realization.

Finally, we can formally assess the performance of a policy $\pi$ via its expected utility,
\begin{align*}
  \favg(\pi) \defeq \E[\Phi,\Pi]{f(E(\pi, \Phi), \Phi)}.
\end{align*}
Then, our goal is to come up with a policy that maximizes the expected utility, subject to a cardinality constraint on the number of items to be picked, $|E(\pi, \Phi)| \leq k$.

\subsection{Monotonicity and Submodularity}
\paragraph{Non-adaptive}
Even in the non-adaptive setting, where the realization is fixed and known in advance, or equivalently, the objective function $f : 2^E \to \mathbb{R}_{\geq 0}$ depends only on the chosen subset, the problem of maximizing $f(A)$, subject to a cardinality constraint $|A| \leq k$, is NP-hard in general.
In this setting, the marginal gain of an element $e \in E$ given set $B \subseteq E$ is defined as $f(B \cup \{e\}) - f(B)$.
Intuitively, the marginal gain quantifies the increase in utility if we add $e$ to our selection, given that we have already picked the elements in $B$.
Function $f$ is called monotone if for any $B \subseteq C \subseteq E$, it holds that $f(B) \leq f(C)$, which is equivalent to saying that the marginal gain is always positive.
Furthermore, $f$ is called submodular, if for any $B \subseteq C \subseteq E$ and any $e \in E \setminus C$, it holds that $f(C \cup \{e\}) - f(C) \leq  f(B \cup \{e\}) - f(B)$.
This means that the marginal gain of any element decreases as the given set increases ($C \supseteq B$); in other words, it expresses a behavior of ``diminishing returns''.

In their famous result, \citet{nemhauser78} showed that, if $f$ is non-negative, monotone, and submodular, then constructing a subset $A$ by picking elements greedily according to their marginal gains, guarantees that $f(A)$ is a ($1 - 1/e$)-approximation to the optimal value.

\paragraph{Adaptive}
In the significantly more complex adaptive setting, the problem of computing an optimal policy is hard to approximate even for seemingly simple classes of objective functions (e.g. linear), as shown by \citet{golovin11}.
However, they also showed that the notions of monotonicity and submodularity can be naturally generalized to this setting, and be used to give similar performance guarantees as in the non-adaptive case.

More concretely, the expected marginal gain of element $e \in E$ given partial realization $\psi$ can be defined as
\begin{align*}
  \D{e}{\psi} \defeq \E[\Phi]{f\big(\mathcal{D}(\psi) \cup \{e\}, \Phi\big) - f\big(\dom(\psi), \Phi\big)}[\Phi \sim \psi].
\end{align*}
The above expression is a conditional expectation, which only considers realizations that are consistent with $\psi$.
Then, the following properties can be defined analogously to their non-adaptive counterparts:
\begin{itemize}
\item $f$ is called adaptive monotone, if $\D{e}{\psi} \geq 0$, for all $e \in E$ and all $\psi$ of positive probability,
\item $f$ is called adaptive submodular, if $\D{e}{\psi'} \leq \D{e}{\psi}$, for all $e \in E \setminus \dom(\psi')$ and all $\psi' \supseteq \psi$.
\end{itemize}
A stronger condition, which is often much easier to verify in practice, is that $f(\cdot, \phi)$ be submodular for any $\phi$, which immediately implies adaptive submodularity of $f$ for any distribution $p(\phi)$.

Given a number of previously selected elements and their corresponding observed states encoded in a partial realization $\psi$, the adaptive greedy policy $\pigr$ selects the element $e \in E \setminus \psi$ that achieves the highest marginal gain $\D{e}{\psi}$, and does so iteratively until $k$ elements have been selected.
\citet{golovin11} showed that under the assumption of adaptive monotonicity and submodularity, $\pigr$ is a $(1-1/e)$-approximation to the optimal policy in terms of expected utility $\favg$.

\section{Adaptive Random Greedy}
While many functions of interest have been shown to satisfy adaptive monotonicity, there are naturally occuring non-monotone functions, for which there is no existing policy with provable performance gurantees.
We now present our proposed adaptive random greedy policy ($\pig$) for maximizing adaptive submodular functions, and prove approximation ratios for both adaptive monotone and non-monotone objectives.

For technical reasons that will become apparent below, let us assume that we always add a set $D$ of $2k - 1$ dummy elements to the ground set, such that, for any $d \in D$, and any partial realization $\psi$, it holds that $\D{d}{\psi} = 0$.
Obviously, these elements do not affect the optimal policy, and may be removed from the solution of any policy, without affecting its expected utility.

The detailed pseudocode of using adaptive random greedy is presented \algoref{alg:rg}.
As discussed before, we are given a ground set and an objective function, as well as a known distribution over realizations $\Phi$.
After computing the marginal gains
The key difference compared to the adaptive greedy policy is shown in \linesref{lin:upd1}{lin:upd2} of the algorithm.
Rather than selecting the element with the largest expected marginal gain, $\pig$ randomly selects an element among the ones with the $k$ largest gains.
The dummy elements added to the ground set ensure that the policy never picks an element with negative expected marginal gain.
Also, note that, although in \algoref{alg:rg} the returned set $A$ contains exactly $k$ elements, since we remove any dummy elements from it, the actual selected set may very well contain less than $k$ elements.

When running the original adaptive greedy policy, non-monotonicity can lead to situations, where selecting the element of maximum marginal gain leads to traps of low utility that cannot be escaped by that policy.
In contrast, the randomness introduced by adaptive random greedy to the selection of each element helps dealing with such traps (on average) and, thus, leads to guaranteed approximation guarantees for the expected utility, even for non-monotone objectives.

\begin{algorithm}[tb]
  \caption{Adaptive random greedy}
  \label{alg:rg}
  \normalsize{
    \begin{algorithmic}[1]
      \REQUIRE ground set $E$, function $f$, distribution $p(\phi)$, cardinality constraint $k$
      %  \ENSURE set $S_k \subseteq E$
      \LET{$A$}{$\varnothing$}
      \LET{$\psi$}{$\varnothing$}
      \FOR{$i = 1$ \TO $k$}
      \STATE Compute $\D{e}{\psi}$, for all $e \in E$ \label{lin:marg}
      \LET{$\Mk(\psi)$}{$\displaystyle\argmax_{S \subseteq E \setminus A,\,|S| = k}\left\{\sum_{e \in S} \D{e}{\psi} \right\}$} \label{lin:upd1}
      \STATE Sample $m$ uniformly at random from $\Mk(\psi)$ \label{lin:upd2}
      \LET{$A$}{$A \cup \{m\}$}
      \STATE Observe $\Phi(m)$
      \LET{$\psi$}{$\psi \cup \left\{\big(m, \Phi(m)\big)\right\}$}
      \ENDFOR
      \STATE Return $A$
    \end{algorithmic}
  }
\end{algorithm}

\paragraph{Theoretical analysis}
More concretely, we now show that the adaptive random greedy policy retains the $1-1/e$ approximation ratio for adaptive monotone submodular objectives, while, at the same time, it achieves a $1/e$ approximation ratio for non-monotone objectives, under the somewhat stronger condition of submodularity for each realization.
As stated in the previous section, this stronger condition implies adaptive submodularity and, in fact, is satisfied in the majority of practical applications, since it is the most common way to prove adaptive submodularity in the first place.
Therefore, we do not consider it a major restriction in the choice of objective functions.
Also, note that our proofs generalize the results of \cite{buchbinder14} for the random greedy algorithm.

Detailed proofs can be found in the longer version of this paper; in what follows, we provide an outline of our analysis.
First, let us define the expected gain of running policy $\pi$ after having obtained a partial realization $\psi$, as
{\small
\begin{align*}
  \D{\pi}{\psi} \defeq \E[\Phi,\Pi]{f(\dom(\psi) \cup E(\pi, \Phi), \Phi) - f(\dom(\psi), \Phi)}[\Phi \sim \psi].
\end{align*}}
Also, for any policy $\pi$, we define a truncated policy $\pik$, which runs indentically to $\pi$ for $k$ steps and then terminates.
Finally, given any two policies $\pi_1$, $\pi_2$, we denote by $\pi_1@\pi_2$ a new policy that first runs $\pi_1$ until it terminates and then continues with $\pi_2$, discarding the observations made by $\pi_1$.
In the end, the subset selected by $\pi_1@\pi_2$ will be the union of the subsets selected by each policy individually.

The following is a key lemma for both monotone and non-monotone objectives.
\setcounter{lemma}{0}
\begin{lemma}
  If $f$ is adaptive submodular, then, for any policy $\pi$, and any partial realization $\psi$, it holds that
  \begin{align*}
    \D{\pik}{\psi} \leq \sum_{e \in \Mk(\psi)} \D{e}{\psi}.
  \end{align*}
\end{lemma}
\noindent It states the intuitive fact that, at any point, no $k$-step policy can give us a larger expected gain than the sum of the $k$ currently largest expected marginal gains.
This is a consequence of adaptive submodularity, which guaratees that the expected marginal gains of any element will decrease as our selection grows larger.

Based on \lemmaref{lem:submod} and the fact that $\pig$ selects at each step one of the $k$ elements in $\Mk$ uniformly at random, we can show the following lemma, which also applies to both monotone and non-monotone objectives.
\begin{lemma}\label{lem:mon_main}
  For any policy $\pi$ and any non-negative integer $i < k$, if $f$ is adaptive submodular, then
  \begin{align*}
    \favg(\pigii) - \favg(\pigi) \geq \frac{1}{k}\left(\favg(\pigi @ \pi) - \favg(\pigi)\right).
  \end{align*}
\end{lemma}
\noindent The lemma compares the expected gain at the $i$-th step of $\pig$ to the total gain of running any other policy (e.g., the optimal one) after the $i$-th step and, thereby, provides a means for obtaining approximation guarantees for $\pig$, as long as we can bound the term $\favg(\pigi @ \pi)$.

If $f$ is adaptive monotone, we may use the bound $\favg(\pigi @ \pi) \geq \favg(\pi)$, which readily follows from the definition of adaptive monotonicity, to obtain the following theorem.
\begin{theorem}
  If $f$ is adaptive monotone submodular, then for any policy $\pi$ and all integers $i, k > 0$ it holds that
  \begin{align*}
    \favg(\pigi) \geq \left(1 - e^{-i/k}\right)\favg(\pik).
  \end{align*}
\end{theorem}
\noindent In particular, by setting $i = k$ we get the familiar $1-1/e$ approximation ratio for $\pigk$.

For the non-monotone case, we need to leverage the randomness of the selection process of $\pig$ to bound $\favg(\pigi @ \pi)$.
For that purpose, we generalize to the adaptive setting the following lemma shown by \citet{buchbinder14}, which itself is based on a lemma by \citet{feige07} for the expected value of a submodular function under a randomly selected subset.
\begin{lemma}[\cite{buchbinder14}]
  If $f : 2^E \to \mathbb{R}_{\geq 0}$ is submodular and $A$ is a random subset of $E$, such that each element $e \in E$ is contained in $A$ with probability at most $p$, that is, $\P[A]{e \in A} \leq p,\ \forall e \in E$, then
  \begin{align*}
    \E[A]{f(A)} \geq (1-p)f(\varnothing).
  \end{align*}
\end{lemma}
\noindent Roughly speaking, the lemma states that a ``random enough'' subset $A$ cannot have much worse value than that of the empty set.
Note that $f$ here is not assumed to be monotone.

The following lemma extends the above claim to the adaptive setting.
\begin{lemma}
  If $f(\cdot\,, \phi) : 2^E \to \mathbb{R}_{\geq 0}$ is submodular for all $\phi \in O^E$, then for any policy $\pi$ such that each element of $e \in E$ is selected by it with probability at most $p$, that is, $\P[\Pi]{e \in E(\pi, \phi)} \leq p,\ \forall \phi \in O^E,\ \forall e \in E$, the expected value of running $\pi$ can be bounded as follows:
\begin{align*}
  \favg(\pi) \geq (1-p)\,\favg(\pio).
\end{align*}
\end{lemma}
\noindent Although we have not found concrete examples of adaptive submodular functions that do not satisfy this lemma, adaptive submodularity by itself does not seem to be a sufficient condition for it to hold; rather, we require the stronger assumption that $f$ be submodular for any realization.

As a consequence of the above lemma, we get that $\favg(\pigi@\pi) = \favg(\pi@\pigi) \geq (1-p)\favg(\pi)$, meaning that the elements added by adaptive random greedy cannot reduce the average value obtained by any other policy $\pi$ by too much.
The probability $p$ in this case can be bounded by using the fact that $\pig$ randomly selects one of $k$ elements at each step, therefore at the $i$-step we have $p \leq (1-1/k)^i$.
Putting it all together, we obtain the following theorem for non-monotone objectives.
\begin{theorem}\label{thm:nonm}
  If $f(\cdot\,, \phi) : 2^E \to \mathbb{R}_{\geq 0}$ is submodular for all $\phi \in O^E$, then, for any policy $\pi$, and all integers $i, k > 0$, it holds that
  \begin{align*}
    \favg(\pigi) \geq \frac{i}{k}\left(1 - \frac{1}{k}\right)^{i-1}\favg(\pik).
  \end{align*}
\end{theorem}
\noindent By setting $i = k$ we get a $1/e$ approximation ratio for $\pigk$.

\section{Examples of Non-Monotone Objectives}
To underline the importance of non-monotonicity, we now present two different ways, in which non-monotone adaptive submodular functions commonly arise in practice.
We also illustrate them through two representative example objectives, which are themselves of practical interest.

\subsection{Objectives with a Modular Cost Term}
Assume that we are given an adaptive monotone submodular function $f_{\textrm{utility}}(A, \phi)$.
In practice, apart from benefit, there might also be some associated cost with the selection of each element, which can be directly incorporated into the objective function via a modular cost term a cost term $f_{\textrm{cost}}(A) = \sum_{a \in A} c_a$.
In this case, the resulting objective is of the form
\begin{align} \label{eq:cost}
  f(A, \phi) = f_{\textrm{utility}}(A, \phi) - f_{\textrm{cost}}(A),
\end{align}
which is also adaptive submodular, but non-monotone.

\paragraph{Influence maximization}
The concept of influence maximization in a social network was posed by \citet{kempe03} and has direct applications to problems such as viral marketing.
Given a graph and a model of influence propagation, the goal is to select a subset of nodes that are initially active, in order to maximize the spread of influence measured by the expected number of nodes that will ultimately be active according to the propagation model.
We focus here on the independent cascade model, according to which, each edge of the graph is randomly either ``live'' or ``blocked'', independently of any other edge in the network, and influence can only flow along ``live'' edges.

For this problem, the ground set $E$ consists of network nodes and each realization $\Phi$ corresponds to a full outcome of the independent cascade model, that is, a boolean assignment to each network edge of being either ``live'' or ``blocked''.
The objective function $f_{\textrm{inf}}(A, \Phi)$ is the number of ultimately active nodes under realization $\Phi$, if the nodes in the selected subset $A$ are initially active.
In the adaptive version of the problem, when a node $v$ is selected, it reveals the status (``live'' or ``blocked'') of all outgoing edges of $v$ and of any other node that can be reached from $v$ through ``live'' edges.
\citet{kempe03} showed that $f_{\textrm{inf}}$ is non-negative, monotone, and submodular, for any realization $\Phi$, and \citet{golovin11} showed that it is also adaptive monotone submodular.

Now, assume that each selected node incurs a unit cost, that is, we have a cost term $f_{\textrm{cost}}(A) = |A|$, which results in the following objective:
\begin{align*}
  \tilde{f}_{\textrm{inf}}(A, \Phi) \defeq f_{\textrm{inf}}(A, \Phi) - |A|.
\end{align*}
Since our cost term is modular, $\tilde{f}_{\textrm{inf}}$ has the form of equation \eqref{eq:cost}, and, therefore, is non-monotone adaptive submodular.
It is also non-negative, since $f_{\textrm{inf}}(A, \Phi) \geq |A|$.

\subsection{\todo{Submodular for each Realization over Distributions that Factorize?}}
\todo{Use what was shown by \cite{golovin11} or prove more general?}

\paragraph{Maximum graph cut}
The problem of finding the maximum cut in a graph $(\mathcal{V}, \mathcal{E})$ is a much studied problem, for which the objective
\begin{align*}
  f_{\textrm{cut}}(A) = \sum_{(v, w) \in \mathcal{E}} \mathds{1}_{\left\{v \in A, w \in \mathcal{V}\setminus A\ \textrm{or}\ v \in \mathcal{V}\setminus A, w \in A\right\}}
\end{align*}
is non-negative, symmetric submodular, and, therefore, non-monotone.
We consider here an adaptive version of this problem, in which, selecting a node results in cutting that node or either of its neighbors with equal probability.

The ground set $E$ consists of the network nodes, and each realization $\Phi$ corresponds to a function $\sigma_{\Phi}$ that maps each node $v \in V$ to the node that is actually cut if $v$ is selected.
Our modified objective can then be written as follows:
\begin{align*}
  \tilde{f}_{\textrm{cut}}(A, \Phi) \defeq f_{\textrm{cut}}\left(\bigcup_{v \in A}\sigma_{\Phi}(v)\right).
\end{align*}
The new objective is non-negative, non-monotone, and submodular, for any realization $\Phi$. \todo{Show that $\tilde{f}$ is adaptive submodular.}

\section{Experiments}
We have evaluated our proposed algorithm on the two objective functions described in the previous section using both synthetic and real-world data sets.
Since we have no competitor policy for the adaptive non-monotone submodular setting, we rather focus here on showcasing the potential benefits of adaptivity by comparing adaptive to non-adaptive random greedy on these two functions that are directly applicable to both settings and satisfy the assumptions of \theoremref{thm:nonm}.

\subsection{Data sets}

\subsection{Results}

\section{Related work}
\paragraph{Non-monotone submodular maximization}\\
\paragraph{Adaptive submodularity}\\
\paragraph{Influence maximization and max-cut}

\section{Conclusion}

\clearpage
% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai15}

\iftoggle{short}
{}
{
\clearpage
\onecolumn
\appendix
\setcounter{lemma}{0}
\setcounter{theorem}{0}
\include{proofs}
}

\end{document}
