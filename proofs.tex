%\chapter{Proofs}

\section{Preliminaries}

\subsection{Definitions}
\begin{description}[labelindent=0pt,leftmargin=7pt,itemindent=-2pt,itemsep=0pt]
  \uitem{Ground set} $E$
  \uitem{Observation set} $O$
  \uitem{Realization} $\phi : E \to O$
  \uitem{Partial realization} $\psi \subseteq E \times O$ with $\psi(e) \defeq o,\ \forall (e, o) \in \psi$
  \uitem{Partial realiz. domain} $\dom(\psi) \defeq \sdef{e \in E}{\exists o \in O : (e, o) \in \psi}$
  \uitem{Consistency} $\phi \sim \psi \iff \phi(e) = \psi(e),\ \forall e \in \dom(\psi)$
  \uitem{Subrealization} $\psi_1 \subseteq \psi_2$
  \uitem{Probability simplex} $\mathcal{P}(E) \defeq \sdef{\*x \in \mathbb{R}^E}{\sum_{e \in E} x_e = 1,\ x_e \geq 0\ \forall e \in E}$
  \uitem{Policy} $\pi \subseteq 2^{E \times O} \times \mathcal{P}(E)$ with $\pi(e\mid\psi) \defeq p(e),\ \forall (\psi, p) \in \pi$
  \uitem{Policy domain} $\dom(\pi) \defeq \sdef{\psi \in 2^{E \times O}}{\exists p \in \mathcal{P}(E) : (\psi, p) \in \pi}$
  \uitem{Truncated policy} $\pik \subseteq 2^{E \times O} \times \mathcal{P}(E)$ such that $\dom(\pik) = \sdef{\psi \in \dom(\pi)}{|\psi| < k}$ and $\pik(\psi) = \pi(\psi),\ \forall \psi \in \dom(\pik)$
  \uitem{Policy concatenation} $\pi_1 @ \pi_2$
  \uitem{Selected items} $E(\pi, \phi) \subseteq E$
  \uitem{Function} $f : 2^E \times O^E \to \mathbb{R}_{\geq 0}$
  \uitem{Exp. value of policy} $\favg(\pi) \defeq \E[\Phi,\Pi]{f(E(\pi, \Phi), \Phi)}$
  \uitem{Expected gain of element} $\D{e}{\psi} \defeq \E[\Phi]{f(\dom(\psi) \cup \{e\}, \Phi) - f(\dom(\psi), \Phi)}[\Phi \sim \psi]$
  \uitem{Expected gain of policy} $\D{\pi}{\psi} \defeq \E[\Phi,\Pi]{f(\dom(\psi) \cup E(\pi, \Phi), \Phi) - f(\dom(\psi), \Phi)}[\Phi \sim \psi]$
  \uitem{Random greedy policy} $\pig$
  \uitem{Random greedy set} $\mathcal{M}_k(\psi) \in \displaystyle\argmax_{S \subseteq E, |S| \leq k}\left\{\sum_{e \in S} \D{e}{\psi} \right\}$
\end{description}

\subsection{Problem statement}
We consider the problem of adaptive maximization, i.e., of finding a policy $\pi$ that solves the following problem
\begin{align*}
  \textrm{maximize}&\quad  \favg(\pi)\\
  \textrm{subject to}&\quad  |E(\pi, \phi)| \leq k,\ \forall \phi \in O^E.
\end{align*}

\subsection{Assumptions}

\begin{assumption}\label{as:nonneg}
\todo{Need to assume non-negativity of $f$ (in expectation or for every $\phi$?)}
\end{assumption}

\section{Adaptive monotone submodular}

\begin{lemma}\label{lem:mon_subm}
  For any partial realization $\psi$, let $\mathcal{M}_k(\psi)$ be a set containing the $k$ elements with the largest expected marginal gains.
  If $f$ is adaptive submodular, then for any policy $\pi$ it holds that
  \begin{align*}
    \D{\pik}{\psi} \leq \sum_{e \in \mathcal{M}_k(\psi)} \D{e}{\psi}.
  \end{align*}
\end{lemma}
\begin{proof}
  This is a more detailed version of the proof of Lemma A.9 of~\citet{golovin11}.
  First note that by definition of $\pik$ it holds that $|E(\pik, \phi)| \leq k,$ for all $\phi$, which also implies that $\E[\Phi,\Pi]{|E(\pik, \Phi)|} \leq k$.
  Let $p(e)$ denote the probability that element $e$ will be selected by the truncated policy $\pik$, that is, $p(e) \defeq \P[\Phi,\Pi]{e \in E(\pik, \Phi)}$.
  Then, we have
  \begin{align}
    \label{eq:pre}
    \begin{split}
      k &\geq \E[\Phi, \Pi]{|E(\pik, \Phi)|}\\
        &= \sum_{e \in E}\P[\Phi,\Pi]{e \in E(\pik, \Phi)}\\
        &= \sum_{e \in E}p(e).
    \end{split}
  \end{align}
  Now, consider the following fractional knapsack problem:
  \begin{align*}
    \textrm{maximize} & \quad g(\*w) \defeq \sum_{e \in E}\D{e}{\psi}w_e\\
    \textrm{subject to} & \quad \sum_{e \in E}w_e \leq k\\
               & \quad 0 \leq w_e \leq 1,\ \forall e \in E.
  \end{align*}
  Note that by \eqref{eq:pre} and the fact that $p(e)$ are probabilities, it follows that $\*p \defeq \left(p(e),\ e \in E\right)$ is a feasible vector for the above problem.
  Furthermore, the vector $\*m$ defined by
  \begin{align*}
    m_e = \twopartdefo{1}{e \in \mathcal{M}_k(\psi)}{0}
  \end{align*}
  is an optimal solution. \todo{Could add a proof by contradiction here if this isn't obvious.}
  Therefore, we have
  \begin{align}\label{eq:knap}
                    &\ g(\*p) \leq g(\*m) \notag\\
    \Leftrightarrow &\ \sum_{e \in E}\D{e}{\psi}w_e \leq \sum_{e \in \mathcal{M}_k(\psi)} \D{e}{\psi}
  \end{align}
  
  If we define the probability that partial realization $\psi' \supseteq \psi$ will come up when running policy $\pik$ as $p(\psi'\mmid\psi) \defeq \P[\Phi,\Pi]{E(\pik, \Phi) \supseteq \dom(\psi')}[\Phi \sim \psi]$, then the gain $\D{\pik}{\psi}$ can be bounded as follows:
  \begin{align*}
    \D{\pik}{\psi} &= \sum_{\psi' \in \dom(\pik)}p(\psi'\mmid\psi)\sum_{e \in E}\pi(e\mmid\psi')\D{e}{\psi'}\\
    &\leq \sum_{\psi' \in \dom(\pik)}p(\psi'\mmid\psi)\sum_{e \in E}\pi(e\mmid\psi')\D{e}{\psi} \tag*{(by AS)}\\
    &= \sum_{e \in E}\D{e}{\psi} \sum_{\psi' \in \dom(\pik)}\pi(e\mmid\psi')p(\psi'\mmid\psi)\\
    &= \sum_{e \in E}\D{e}{\psi} p(e)\\
    &\leq \sum_{e \in \mathcal{M}_k(\psi)} \D{e}{\psi} \tag*{(by \eqref{eq:knap})}.
  \end{align*}
\end{proof}

\begin{lemma}\label{lem:mon_main}
  For any policy $\pi$ and any non-negative integer $i < k$, if $f$ is adaptive submodular, the expected marginal gain obtained at the $i$-th step of random greedy policy $\pig$ can bounded as
  \begin{align*}
    \favg(\pigii) - \favg(\pigi) \geq \frac{1}{k}\left(\favg(\pigi @ \pi) - \favg(\pigi)\right).
  \end{align*}
\end{lemma}
\begin{proof}
  Fix $i < k$ and let $\Psi$ be a random variable denoting the partial realization that results from running the random greedy policy for $i$ steps, distributed as
  \begin{align*}
    \P[\Psi]{\Psi = \psi} = \P[\Phi, \Pi]{E(\pigi, \Phi) = \dom(\psi)}[\Phi \sim \psi].
  \end{align*}
  Also, let $U_i$ be a random variable denoting the element chosen at the $i$-th step of the random greedy policy. Due to the way the random greedy policy selects the next element at each step, the distribution of $U_{i+1}$ conditioned on some partial realization $\psi$ up to step $i$ is
  \begin{align}\label{eq:ui}
    \P[\Pi]{U_{i+1} = e} = \twopartdefo{1/k}{e \in \mathcal{M}_k(\psi)}{0}.
  \end{align}

  Then, for the expected marginal gain at the $i$-th step we have
  \begin{align*}
     &\ \favg(\pigii) - \favg(\pigi)\\
    =&\ \E[\Phi, \Pi]{f(E(\pigii, \Phi), \Phi) - f(E(\pigi, \Phi), \Phi)}\\
    =&\ \E[\Psi, \Phi, \Pi]{f(\dom(\Psi) \cup \{U_{i+1}\}, \Phi) - f(\dom(\Psi), \Phi)}[\Phi \sim \Psi]\\
    =&\ \E[\Psi, \Phi]{\sum_{e \in \mathcal{M}_k(\Psi)}\frac{1}{k}\left[f(\dom(\Psi) \cup \{e\}, \Phi) - f(\dom(\Psi), \Phi)\right]}[\Phi \sim \Psi] \tag*{(by \eqref{eq:ui})}\\
    =&\ \frac{1}{k}\E[\Psi]{\sum_{e \in \mathcal{M}_k(\Psi)}\D{e}{\Psi}}\\
    \geq&\ \frac{1}{k}\E[\Psi]{\D{\pi}{\Psi}} \tag*{(by~\lemmaref{lem:mon_subm})}\\
    =&\ \frac{1}{k}\E[\Psi, \Phi]{f(\dom(\Psi) \cup E(\pi, \Phi), \Phi) - f(\dom(\Psi), \Phi)}[\Phi \sim \Psi]\\
    =& \ \frac{1}{k}\left(\favg(\pigi @ \pi) - \favg(\pigi)\right).
  \end{align*}
\end{proof}

\begin{lemma}\label{lem:mon_mon}
  Function $f$ is adaptive monotone if and only if for all policies $\pi_1$ and $\pi_2$ it holds that $\favg(\pi_2) \leq \favg(\pi_1 @ \pi_2)$.
\end{lemma}
\begin{proof}
See Lemma A.8 of \citet{golovin11}.
\end{proof}

\begin{theorem}
  If $f$ is adaptive monotone submodular, then for any policy $\pi$ and all integers $i, k \geq 0$ it holds that
  \begin{align*}
    \favg(\pigi) \geq \left(1 - e^{-i/k}\right)\favg(\pik).
  \end{align*}
\end{theorem}
\begin{proof}
  By combining \lemmasref{lem:mon_main} and~\ref{lem:mon_mon} it immediately follows that for all $i$, $k \geq 0$
  \begin{align*}
                   &\ \favg(\pigii) - \favg(\pigi) \geq \frac{1}{k}\left(\favg(\pik) - \favg(\pigi)\right)\\
    \Leftrightarrow&\ \favg(\pigii) \geq \frac{1}{k}\favg(\pik) + \left(1 - \frac{1}{k}\right) \favg(\pigi)\\
    \Leftrightarrow&\ \favg(\pik) - \favg(\pigii) \leq \left(1 - \frac{1}{k}\right)\left(\favg(\pik) - \favg(\pigi)\right)\\
    \Leftrightarrow&\ \favg(\pik) - \favg(\pigi) \leq \left(1 - \frac{1}{k}\right)^{i}\left(\favg(\pik) - \favg(\pigo)\right)\\
    \Leftrightarrow&\ \favg(\pigi) \geq \left(1 - \left(1 - \frac{1}{k}\right)^{i}\right)\favg(\pik)\tag*{(by \asref{as:nonneg})}\\
    \Leftrightarrow&\ \favg(\pigi) \geq \left(1 - e^{-i/k}\right)\favg(\pik)\tag*{($1 - x \leq e^{-x},\ \forall x \geq 0$)}
  \end{align*}
\end{proof}

\begin{cor}
If $f$ is adaptive monotone submodular, then for any policy $\pi$ and any non-negative integer $k$ it holds that
  \begin{align*}
    \favg(\pigk) \geq (1 - \frac{1}{e})\favg(\pik).
  \end{align*}
\end{cor}