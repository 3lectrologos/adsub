%\chapter{Proofs}

\section{Definitions}

\begin{itemize}
  \uitem{Ground set} $E$
  \uitem{Observation set} $O$
  \uitem{Realization} $\phi : E \to O$
  \uitem{Partial realization} $\psi \subseteq E \times O$ with $\psi(e) \defeq o,\ \forall (e, o) \in \psi$
  \uitem{Partial realiz. domain} $\dom(\psi) \defeq \sdef{e \in E}{\exists o \in O : (e, o) \in \psi}$
  \uitem{Consistency} $\phi \sim \psi \iff \phi(e) = \psi(e),\ \forall e \in \dom(\psi)$
  \uitem{Subrealization} $\psi_1 \subseteq \psi_2$
  \uitem{Probability simplex} $\mathcal{P}(E) \defeq \sdef{\*x \in \mathbb{R}^E}{\sum_{e \in E} x_e = 1,\ x_e \geq 0\ \forall e \in E}$
  \uitem{Policy} $\pi \subseteq 2^{E \times O} \times \mathcal{P}(E)$ with $\pi(e\mid\psi) \defeq p(e),\ \forall (\psi, p) \in \pi$
  \uitem{Policy domain} $\dom(\pi) \defeq \sdef{\psi \in 2^{E \times O}}{\exists p \in \mathcal{P}(E) : (\psi, p) \in \pi}$
  \uitem{Truncated policy} $\pik \subseteq 2^{E \times O} \times \mathcal{P}(E)$ such that $\dom(\pik) = \sdef{\psi \in \dom(\pi)}{|\psi| < k}$ and $\pik(\psi) = \pi(\psi),\ \forall \psi \in \dom(\pik)$
  \uitem{Policy concatenation} $\pi_1 @ \pi_2$
  \uitem{Selected items} $E(\pi, \phi) \subseteq E$
  \uitem{Function} $f : 2^E \times O^E \to \mathbb{R}_{\geq 0}$
  \uitem{Exp. value of policy} $\favg(\pi) \defeq \E[\Phi,\Pi]{f(E(\pi, \Phi), \Phi)}$
  \uitem{Marginal gain of $e$} $\D{e}{\psi} \defeq \E[\Phi]{f(\dom(\psi) \cup \{e\}, \Phi) - f(\dom(\psi), \Phi)}[\Phi \sim \psi]$
  \uitem{Marginal gain of $\pi$} $\D{\pi}{\psi} \defeq \E[\Phi,\Pi]{f(\dom(\psi) \cup E(\pi, \Phi), \Phi) - f(\dom(\psi), \Phi)}[\Phi \sim \psi]$
  \uitem{Random greedy set} $\mathcal{M}_k(\psi) \in \displaystyle\argmax_{S \subseteq E, |S| = k}\left\{\sum_{e \in S} \D{e}{\psi} \right\}$
\end{itemize}


\section{Adaptive monotone submodular}

\todo{Need to add dummy elements, so that it's always possible to choose $k$ of them with non-negative expected marginal gains.}

\begin{lemma}\label{lem:mon_subm}
  For any partial realization $\psi$, let $\mathcal{M}_k(\psi)$ be a set containing the $k$ elements with the largest expected marginal gains.
  If $f$ is adaptive submodular, then for any policy $\pi$ it holds that
  \begin{align*}
    \D{\pik}{\psi} \leq \sum_{e \in \mathcal{M}_k(\psi)} \D{e}{\psi}.
  \end{align*}
\end{lemma}
\begin{proof}
  First note that by definition of $\pik$ it holds that $|E(\pik, \phi)| \leq k,$ for all $\phi$, which also implies that $\E[\Phi,\Pi]{|E(\pik, \Phi)|} \leq k$.
  Let $p(e)$ denote the probability that element $e$ will be selected by the truncated policy $\pik$, that is, $p(e) \defeq \P[\Phi,\Pi]{e \in E(\pik, \Phi)}$.
  Then, we have
  \begin{align}
    \label{eq:pre}
    \begin{split}
      k &\geq \E[\Phi, \Pi]{|E(\pik, \Phi)|}\\
        &= \sum_{e \in E}\P[\Phi,\Pi]{e \in E(\pik, \Phi)}\\
        &= \sum_{e \in E}p(e).
    \end{split}
  \end{align}
  If we define $p(\psi'\mmid\psi) \defeq \P[\Phi,\Pi]{E(\pik, \Phi) \supseteq \dom(\psi')}[\Phi \sim \psi]$, i.e., the probability that partial realization $\psi' \supseteq \psi$ will come up when running policy $\pik$, the gain $\D{\pik}{\psi}$ can be bounded as follows:
  \begin{align*}
    \D{\pik}{\psi} &= \sum_{\psi' \in \dom(\pik)}p(\psi'\mmid\psi)\sum_{e \in E}\pi(e\mmid\psi')\D{e}{\psi'}\\
    &\leq \sum_{\psi' \in \dom(\pik)}p(\psi'\mmid\psi)\sum_{e \in E}\pi(e\mmid\psi')\D{e}{\psi} \tag*{(by AS)}\\
    &= \sum_{e \in E}\D{e}{\psi} \sum_{\psi' \in \dom(\pik)}\pi(e\mmid\psi')p(\psi'\mmid\psi)\\
    &= \sum_{e \in E}\D{e}{\psi} p(e)\\
    &\leq \sum_{e \in \mathcal{M}_k(\psi)} \D{e}{\psi}.
  \end{align*}
  The last inequality above follows from considering the fractional knapsack problem
  \begin{align*}
    \textrm{maximize} & \quad \sum_{e \in E}p(e)\D{e}{\psi}\\
    \textrm{subject to} & \quad \sum_{e \in E}p(e) \leq k \tag*{(by \eqref{eq:pre})}\\
               & \quad 0 \leq p(e) \leq 1,\ \forall e \in E \tag*{($p(e)$ are probabilities)}
  \end{align*}
  and noting that $\mathcal{M}_k(\psi)$ is a maximizer, since Assumption 1 guarantees that all elements in  $\mathcal{M}_k(\psi)$ have non-negative expected marginal gains, that is, $\D{e}{\psi} \geq 0,\ \forall e \in \mathcal{M}_k(\psi)$.
\end{proof}

\begin{lemma}
  For any policy $\pi$ and any non-negative integer $i < k$, if $f$ is adaptive submodular, the expected marginal gain obtained at the $i$-th step of random greedy policy $\pig$ can bounded as
  \begin{align*}
    \favg(\pigii) - \favg(\pigi) \geq \frac{1}{k}\left(\favg(\pigi @ \pi) - \favg(\pigi)\right).
  \end{align*}
\end{lemma}
\begin{proof}
  Fix $i < k$ and let $\Psi$ be a random variable denoting the partial realization that results from running the random greedy policy for $i$ steps, distributed as
  \begin{align*}
    \P[\Psi]{\Psi = \psi} = \P[\Phi, \Pi]{E(\pigi, \Phi) = \dom(\psi)}[\Phi \sim \psi].
  \end{align*}
  Also, let $U_i$ be a random variable denoting the element chosen at the $i$-th step of the random greedy policy. Due to the way the random greedy policy selects the next element at each step, the distribution of $U_{i+1}$ conditioned on some partial realization $\psi$ up to step $i$ is
  \begin{align}\label{eq:ui}
    \P[\Pi]{U_{i+1} = e} = \twopartdefo{1/k}{e \in \mathcal{M}_k(\psi)}{0}.
  \end{align}

  Then, for the expected marginal gain at the $i$-th step we have
  \begin{align*}
     &\ \favg(\pigii) - \favg(\pigi)\\
    =&\ \E[\Phi, \Pi]{f(E(\pigii, \Phi), \Phi) - f(E(\pigi, \Phi), \Phi)}\\
    =&\ \E[\Psi, \Phi, \Pi]{f(\dom(\Psi) \cup \{U_{i+1}\}, \Phi) - f(\dom(\Psi), \Phi)}[\Phi \sim \Psi]\\
    =&\ \E[\Psi, \Phi]{\sum_{e \in \mathcal{M}_k(\Psi)}\frac{1}{k}\left[f(\dom(\Psi) \cup \{e\}, \Phi) - f(\dom(\Psi), \Phi)\right]}[\Phi \sim \Psi] \tag*{(by \eqref{eq:ui})}\\
    =&\ \frac{1}{k}\,\E[\Psi]{\sum_{e \in \mathcal{M}_k(\Psi)}\D{e}{\Psi}}\\
    \geq&\ \frac{1}{k}\,\E[\Psi]{\D{\pi}{\Psi}} \tag*{(by~\lemmaref{lem:mon_subm})}\\
    =&\ \frac{1}{k}\,\E[\Psi, \Phi]{f(\dom(\Psi) \cup E(\pi, \Phi), \Phi) - f(\dom(\Psi), \Phi)}[\Phi \sim \Psi]\\
    =& \ \frac{1}{k}\,\left(\favg(\pigi @ \pi) - \favg(\pigi)\right).
  \end{align*}
\end{proof}

\begin{lemma}
  Function $f$ is adaptive monotone if and only if for all policies $\pi_1$ and $\pi_2$ it holds that $\favg(\pi_1) \leq \favg(\pi_1 @ \pi_2)$.
\end{lemma}
\begin{proof}
See Lemma A.8 of \citet{golovin11}.
\end{proof}

\begin{theorem}
  If $f$ is adaptive monotone submodular, then for any policy $\pi$ and all non-negative integers $\ell$ and $k$ it holds that
  \begin{align*}
    \favg(\pigl) \geq (1 - e^{-\ell/k})\favg(\pik).
  \end{align*}
\end{theorem}
\begin{proof}
\todo{}
\end{proof}